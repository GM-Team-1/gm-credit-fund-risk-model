{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc7f31c7"
   },
   "source": [
    "# Task\n",
    "Clean the dataset by handling missing values, standardizing categorical variables, and removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original data\n",
    "df = pd.read_csv('../data/companies.csv')\n",
    "\n",
    "# Basic preprocessing\n",
    "df['founded_at'] = pd.to_datetime(df['founded_at'], errors='coerce')\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "\n",
    "current_year = pd.Timestamp.now().year\n",
    "df['founded_year'] = df['founded_at'].dt.year\n",
    "df['company_age_years'] = current_year - df['founded_year']\n",
    "\n",
    "# Handle missing founded years\n",
    "missing_founded = df['founded_year'].isna()\n",
    "df.loc[missing_founded, 'founded_year'] = df.loc[missing_founded, 'created_at'].dt.year\n",
    "df.loc[missing_founded, 'company_age_years'] = current_year - df.loc[missing_founded, 'founded_year']\n",
    "df['company_age_years'] = df['company_age_years'].clip(upper=50, lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Funding features\n",
    "df['funding_total_usd'] = pd.to_numeric(df['funding_total_usd'], errors='coerce')\n",
    "df['has_funding'] = (~df['funding_total_usd'].isna() & (df['funding_total_usd'] > 0)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OPTIMIZED TARGET DISTRIBUTION ===\n",
      "risk_tier_label\n",
      "high_risk      170553\n",
      "low_risk        19976\n",
      "medium_risk      6024\n",
      "Name: count, dtype: int64\n",
      "High risk rate: 86.8%\n",
      "\n",
      "=== VALIDATION BY STATUS ===\n",
      "risk_tier_label  high_risk  low_risk  medium_risk\n",
      "status                                           \n",
      "acquired          0.751756  0.248244     0.000000\n",
      "closed            1.000000  0.000000     0.000000\n",
      "ipo               0.577601  0.422399     0.000000\n",
      "operating         0.873589  0.093572     0.032839\n",
      "\n",
      "=== RISK BY AGE GROUPS ===\n",
      "risk_tier_label  high_risk  low_risk  medium_risk\n",
      "age_group                                        \n",
      "8-15y             0.919854  0.049031     0.031114\n",
      "15+y              0.792937  0.176687     0.030376\n",
      "\n",
      "Saved optimized targets to companies_optimized_targets.csv\n",
      "Final dataset: 196,553 companies with 16 columns\n"
     ]
    }
   ],
   "source": [
    "# Initialize targets\n",
    "df['failure_risk'] = 0\n",
    "df['risk_tier'] = 1  # 0=Low, 1=Medium, 2=High\n",
    "\n",
    "# TIER 0: LOW RISK (Successful/Strong Companies)\n",
    "low_risk_conditions = [\n",
    "    # Successful exits - ALWAYS low risk\n",
    "    df['status'].isin(['ipo', 'acquired']),\n",
    "\n",
    "    # Well-funded operating companies\n",
    "    (df['status'] == 'operating') & (df['funding_total_usd'] > 500000),\n",
    "\n",
    "    # Young companies with decent funding\n",
    "    (df['company_age_years'] <= 3) & (df['funding_total_usd'] > 100000),\n",
    "\n",
    "    # Companies with significant funding regardless of age\n",
    "    (df['funding_total_usd'] > 2000000)\n",
    "]\n",
    "\n",
    "low_risk_mask = pd.concat(low_risk_conditions, axis=1).any(axis=1)\n",
    "df.loc[low_risk_mask, 'risk_tier'] = 0\n",
    "df.loc[low_risk_mask, 'failure_risk'] = 0\n",
    "\n",
    "# TIER 2: HIGH RISK (Clear Failure Signals)\n",
    "high_risk_conditions = [\n",
    "        # Explicitly closed\n",
    "    df['status'] == 'closed',\n",
    "\n",
    "        # Very old with no funding (true zombies)\n",
    "    (df['company_age_years'] > 10) & (df['has_funding'] == 0),\n",
    "\n",
    "        # Old with extremely low funding\n",
    "    (df['company_age_years'] > 8) & (df['funding_total_usd'] < 10000),\n",
    "]\n",
    "\n",
    "high_risk_mask = pd.concat(high_risk_conditions, axis=1).any(axis=1)\n",
    "df.loc[high_risk_mask, 'risk_tier'] = 2\n",
    "df.loc[high_risk_mask, 'failure_risk'] = 1\n",
    "\n",
    "    # TIER 1: MEDIUM RISK (Everything else - the uncertain middle)\n",
    "    # This is automatic based on the initialization\n",
    "\n",
    "    # Map to readable labels\n",
    "risk_labels = {0: 'low_risk', 1: 'medium_risk', 2: 'high_risk'}\n",
    "df['risk_tier_label'] = df['risk_tier'].map(risk_labels)\n",
    "\n",
    "    # Validation\n",
    "print(\"\\n=== OPTIMIZED TARGET DISTRIBUTION ===\")\n",
    "print(df['risk_tier_label'].value_counts().sort_index())\n",
    "print(f\"High risk rate: {df['failure_risk'].mean():.1%}\")\n",
    "\n",
    "    # Validate key segments\n",
    "print(\"\\n=== VALIDATION BY STATUS ===\")\n",
    "status_risk = pd.crosstab(df['status'], df['risk_tier_label'], normalize='index')\n",
    "print(status_risk)\n",
    "\n",
    "    # Analyze risk by age and funding\n",
    "print(\"\\n=== RISK BY AGE GROUPS ===\")\n",
    "df['age_group'] = pd.cut(df['company_age_years'],\n",
    "                            bins=[0, 3, 7, 15, 50],\n",
    "                            labels=['0-3y', '4-7y', '8-15y', '15+y'],\n",
    "                            right=False)\n",
    "age_risk = pd.crosstab(df['age_group'], df['risk_tier_label'], normalize='index')\n",
    "print(age_risk)\n",
    "\n",
    "    # Save optimized version\n",
    "output_columns = [\n",
    "        'id', 'name', 'status', 'category_code', 'country_code', 'state_code', 'region',\n",
    "        'founded_at', 'founded_year', 'company_age_years', 'age_group',\n",
    "        'funding_total_usd', 'has_funding',\n",
    "        'failure_risk', 'risk_tier', 'risk_tier_label'\n",
    "]\n",
    "\n",
    "    # Only include existing columns\n",
    "existing_columns = [col for col in output_columns if col in df.columns]\n",
    "df_output = df[existing_columns]\n",
    "\n",
    "df_output.to_csv('../processed_data/companies_optimized_targets.csv', index=False)\n",
    "print(f\"\\nSaved optimized targets to companies_optimized_targets.csv\")\n",
    "print(f\"Final dataset: {len(df_output):,} companies with {len(existing_columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ababcbac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values per column:\n",
      "parent_id              100.000000\n",
      "ROI                     99.630634\n",
      "last_investment_at      98.685851\n",
      "first_investment_at     98.685851\n",
      "invested_companies      98.681780\n",
      "investment_rounds       98.681780\n",
      "closed_at               98.667026\n",
      "short_description       96.371971\n",
      "funding_total_usd       85.818583\n",
      "last_funding_at         83.970227\n",
      "first_funding_at        83.970227\n",
      "funding_rounds          83.868473\n",
      "state_code              74.102151\n",
      "twitter_username        58.997828\n",
      "tag_list                58.559778\n",
      "lng                     57.338733\n",
      "lat                     57.338733\n",
      "city                    57.319400\n",
      "country_code            55.233448\n",
      "founded_at              53.586564\n",
      "milestones              53.346426\n",
      "last_milestone_at       53.346426\n",
      "first_milestone_at      53.346426\n",
      "description             53.168865\n",
      "logo_height             43.979486\n",
      "logo_url                43.979486\n",
      "logo_width              43.979486\n",
      "category_code           37.326828\n",
      "domain                  35.617874\n",
      "homepage_url            35.617874\n",
      "overview                35.401139\n",
      "relationships           34.029498\n",
      "created_by              20.869689\n",
      "age_group                0.800802\n",
      "normalized_name          0.013228\n",
      "name                     0.011702\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing_percentages = df.isnull().mean() * 100\n",
    "missing_percentages = missing_percentages.sort_values(ascending=False)\n",
    "\n",
    "print(\"Percentage of missing values per column:\")\n",
    "print(missing_percentages[missing_percentages > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped administrative columns: ['entity_type', 'entity_id', 'parent_id', 'Unnamed: 0.1', 'permalink']\n"
     ]
    }
   ],
   "source": [
    "admin_cols_to_drop = [\n",
    "    'entity_type', 'entity_id', 'parent_id',\n",
    "    'Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 1',\n",
    "    'permalink'\n",
    "]\n",
    "\n",
    "existing_admin_cols = [col for col in admin_cols_to_drop if col in df.columns]\n",
    "df = df.drop(columns=existing_admin_cols)\n",
    "print(f\"Dropped administrative columns: {existing_admin_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d849a3eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 5 columns with >50% missing values.\n",
      "Preserved critical columns: ['funding_total_usd', 'funding_rounds', 'first_funding_at', 'last_funding_at', 'country_code', 'state_code', 'city', 'region', 'lat', 'lng', 'category_code', 'description', 'overview', 'tag_list', 'investment_rounds', 'invested_companies', 'milestones', 'first_milestone_at', 'last_milestone_at', 'twitter_username', 'homepage_url', 'domain', 'closed_at']\n"
     ]
    }
   ],
   "source": [
    "# Now handle missing values - drop columns with missing percentage > 50% \n",
    "missing_percentages = df.isnull().mean() * 100\n",
    "missing_percentages = missing_percentages.sort_values(ascending=False)\n",
    "\n",
    "columns_to_drop = missing_percentages[missing_percentages > 50].index\n",
    "\n",
    "critical_cols = [\n",
    "    # Funding-related\n",
    "    'funding_total_usd', 'funding_rounds', 'first_funding_at', 'last_funding_at',\n",
    "    \n",
    "    # Geographic data\n",
    "    'country_code', 'state_code', 'city', 'region', 'lat', 'lng',\n",
    "    \n",
    "    # Company descriptive info (valuable for analysis)\n",
    "    'category_code', 'description', 'overview', 'tag_list',\n",
    "    \n",
    "    # Investment/milestone data\n",
    "    \n",
    "    'investment_rounds', 'invested_companies', 'milestones',\n",
    "    'first_milestone_at', 'last_milestone_at',\n",
    "    \n",
    "    # Company metadata\n",
    "    'twitter_username', 'homepage_url', 'domain', 'closed_at'\n",
    "]\n",
    "\n",
    "columns_to_drop = columns_to_drop.difference(critical_cols)\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "print(f\"Dropped {len(columns_to_drop)} columns with >50% missing values.\")\n",
    "print(f\"Preserved critical columns: {[col for col in critical_cols if col in df.columns]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2bc5f437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns with missing values:\n",
      "invested_companies    193962\n",
      "investment_rounds     193962\n",
      "closed_at             193933\n",
      "funding_total_usd     168679\n",
      "last_funding_at       165046\n",
      "first_funding_at      165046\n",
      "funding_rounds        164846\n",
      "state_code            145650\n",
      "twitter_username      115962\n",
      "tag_list              115101\n",
      "lat                   112701\n",
      "lng                   112701\n",
      "city                  112663\n",
      "country_code          108563\n",
      "first_milestone_at    104854\n",
      "last_milestone_at     104854\n",
      "milestones            104854\n",
      "description           104505\n",
      "logo_height            86443\n",
      "logo_width             86443\n",
      "logo_url               86443\n",
      "category_code          73367\n",
      "homepage_url           70008\n",
      "domain                 70008\n",
      "overview               69582\n",
      "relationships          66886\n",
      "created_by             41020\n",
      "age_group               1574\n",
      "normalized_name           26\n",
      "name                      23\n",
      "dtype: int64\n",
      "\n",
      "Remaining missing values (this is NORMAL and OK):\n",
      "invested_companies    193941\n",
      "investment_rounds     193941\n",
      "closed_at             193911\n",
      "funding_total_usd     168657\n",
      "last_funding_at       165025\n",
      "first_funding_at      165025\n",
      "funding_rounds        164825\n",
      "state_code            145629\n",
      "twitter_username      115945\n",
      "tag_list              115087\n",
      "lng                   112686\n",
      "lat                   112686\n",
      "city                  112648\n",
      "country_code          108550\n",
      "first_milestone_at    104839\n",
      "last_milestone_at     104839\n",
      "milestones            104839\n",
      "description           104492\n",
      "logo_height            86431\n",
      "logo_width             86431\n",
      "logo_url               86431\n",
      "category_code          73357\n",
      "homepage_url           69997\n",
      "domain                 69997\n",
      "overview               69565\n",
      "relationships          66877\n",
      "created_by             41017\n",
      "age_group               1574\n",
      "normalized_name            4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check what columns still have missing values\n",
    "remaining_missing = df.isnull().sum()\n",
    "remaining_missing = remaining_missing[remaining_missing > 0].sort_values(ascending=False)\n",
    "print(f\"\\nColumns with missing values:\")\n",
    "print(remaining_missing)\n",
    "\n",
    "df.dropna(subset=['name'], inplace=True)\n",
    "\n",
    "# Final check\n",
    "final_missing = df.isnull().sum()\n",
    "final_missing = final_missing[final_missing > 0].sort_values(ascending=False)\n",
    "print(f\"\\nRemaining missing values (this is NORMAL and OK):\")\n",
    "print(final_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 164,825 missing funding_rounds values with 0\n",
      "Left 165,025 missing first_funding_at values as NaN (no funding events)\n",
      "Left 165,025 missing last_funding_at values as NaN (no funding events)\n"
     ]
    }
   ],
   "source": [
    "# Handle other funding-related columns\n",
    "funding_cols_to_fill = ['funding_rounds', 'first_funding_at', 'last_funding_at']\n",
    "for col in funding_cols_to_fill:\n",
    "    if col in df.columns:\n",
    "        if col == 'funding_rounds':\n",
    "            # Missing funding rounds = 0 rounds\n",
    "            before_count = df[col].isnull().sum()\n",
    "            df[col] = df[col].fillna(0)\n",
    "            print(f\"Imputed {before_count:,} missing {col} values with 0\")\n",
    "        else:\n",
    "            # For date columns, leave as NaN (indicates no funding events)\n",
    "            print(f\"Left {df[col].isnull().sum():,} missing {col} values as NaN (no funding events)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 193,941 missing investment_rounds values with 0\n",
      "Imputed 193,941 missing invested_companies values with 0\n"
     ]
    }
   ],
   "source": [
    "# Handle investment-related columns\n",
    "investment_cols_to_fill = ['investment_rounds', 'invested_companies']\n",
    "for col in investment_cols_to_fill:\n",
    "    if col in df.columns:\n",
    "        before_count = df[col].isnull().sum()\n",
    "        df[col] = df[col].fillna(0)\n",
    "        print(f\"Imputed {before_count:,} missing {col} values with 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 104,839 missing milestones values with 0\n"
     ]
    }
   ],
   "source": [
    "# Handle milestone columns\n",
    "milestone_cols = ['milestones']\n",
    "for col in milestone_cols:\n",
    "    if col in df.columns:\n",
    "        before_count = df[col].isnull().sum()\n",
    "        df[col] = df[col].fillna(0)\n",
    "        print(f\"Imputed {before_count:,} missing {col} values with 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 108,550 missing country_code values with 'Unknown'\n",
      "Imputed 145,629 missing state_code values with 'Unknown'\n",
      "Imputed 0 missing region values with 'Unknown'\n",
      "Imputed 112,648 missing city values with 'Unknown'\n",
      "Imputed 73,357 missing category_code values\n",
      "Imputed 104,492 missing description values\n",
      "Imputed 69,565 missing overview values\n",
      "Imputed 115,087 missing tag_list values\n",
      "Imputed 115,945 missing twitter_username values with 'None'\n",
      "Imputed 69,997 missing homepage_url values with 'None'\n",
      "Imputed 69,997 missing domain values with 'None'\n",
      "Imputed 66,877 missing relationships values with 'Unknown'\n",
      "Imputed 41,017 missing created_by values with 'Unknown'\n",
      "Keeping 112,686 missing lat values as NaN (no fake coordinates)\n",
      "Keeping 112,686 missing lng values as NaN (no fake coordinates)\n",
      "Imputed 86,431 missing logo_height values with median: 105.0\n",
      "Imputed 86,431 missing logo_width values with median: 267.0\n",
      "Dropped 0 rows with missing values in 'name'.\n",
      "\n",
      "Percentage of missing values per column after cleaning:\n",
      "closed_at             98.667379\n",
      "funding_total_usd     85.817432\n",
      "last_funding_at       83.969369\n",
      "first_funding_at      83.969369\n",
      "lat                   57.337811\n",
      "lng                   57.337811\n",
      "first_milestone_at    53.345036\n",
      "last_milestone_at     53.345036\n",
      "logo_url              43.978527\n",
      "age_group              0.800896\n",
      "normalized_name        0.002035\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "geographic_cols = ['country_code', 'state_code', 'region', 'city']\n",
    "for col in geographic_cols:\n",
    "    if col in df.columns:\n",
    "        before_count = df[col].isnull().sum()\n",
    "        df[col] = df[col].fillna('Unknown')\n",
    "        print(f\"Imputed {before_count:,} missing {col} values with 'Unknown'\")\n",
    "\n",
    "text_cols = ['category_code', 'description', 'overview', 'tag_list']\n",
    "for col in text_cols:\n",
    "    if col in df.columns:\n",
    "        before_count = df[col].isnull().sum()\n",
    "        if col == 'category_code':\n",
    "            df[col] = df[col].fillna('other')\n",
    "        else:\n",
    "            df[col] = df[col].fillna('Unknown')\n",
    "        print(f\"Imputed {before_count:,} missing {col} values\")\n",
    "\n",
    "web_cols = ['twitter_username', 'homepage_url', 'domain']\n",
    "for col in web_cols:\n",
    "    if col in df.columns:\n",
    "        before_count = df[col].isnull().sum()\n",
    "        df[col] = df[col].fillna('None')\n",
    "        print(f\"Imputed {before_count:,} missing {col} values with 'None'\")\n",
    "\n",
    "relationship_cols = ['relationships', 'created_by']\n",
    "for col in relationship_cols:\n",
    "    if col in df.columns:\n",
    "        before_count = df[col].isnull().sum()\n",
    "        df[col] = df[col].fillna('Unknown')\n",
    "        print(f\"Imputed {before_count:,} missing {col} values with 'Unknown'\")\n",
    "\n",
    "numerical_cols = ['lat', 'lng', 'logo_height', 'logo_width']\n",
    "for col in numerical_cols:\n",
    "    if col in df.columns:\n",
    "        before_count = df[col].isnull().sum()\n",
    "        if col in ['lat', 'lng']:\n",
    "            # For coordinates, keep as NaN\n",
    "            print(f\"Keeping {before_count:,} missing {col} values as NaN (no fake coordinates)\")\n",
    "        else:\n",
    "            median_value = df[col].median()\n",
    "            df[col] = df[col].fillna(median_value)\n",
    "            print(f\"Imputed {before_count:,} missing {col} values with median: {median_value}\")\n",
    "\n",
    "critical_missing_cols = ['name']\n",
    "for col in critical_missing_cols:\n",
    "    if col in df.columns:\n",
    "        rows_before = len(df)\n",
    "        df.dropna(subset=[col], inplace=True)\n",
    "        rows_after = len(df)\n",
    "        print(f\"Dropped {rows_before - rows_after} rows with missing values in '{col}'.\")\n",
    "\n",
    "# Verify remaining missing values\n",
    "missing_percentages_after = df.isnull().mean() * 100\n",
    "missing_percentages_after = missing_percentages_after.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nPercentage of missing values per column after cleaning:\")\n",
    "print(missing_percentages_after[missing_percentages_after > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "845adab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns identified:\n",
      "Index(['id', 'name', 'normalized_name', 'category_code', 'status', 'closed_at',\n",
      "       'domain', 'homepage_url', 'twitter_username', 'logo_url', 'description',\n",
      "       'overview', 'tag_list', 'country_code', 'state_code', 'city', 'region',\n",
      "       'first_funding_at', 'last_funding_at', 'first_milestone_at',\n",
      "       'last_milestone_at', 'relationships', 'created_by', 'updated_at',\n",
      "       'risk_tier_label'],\n",
      "      dtype='object')\n",
      "\n",
      "Unique values for 'category_code':\n",
      "['web' 'games_video' 'network_hosting' 'advertising' 'cleantech' 'other'\n",
      " 'enterprise' 'consulting' 'mobile' 'health' 'software' 'analytics'\n",
      " 'finance' 'education' 'medical' 'manufacturing' 'biotech' 'ecommerce'\n",
      " 'public_relations' 'hardware' 'search' 'news' 'government' 'security'\n",
      " 'photo_video' 'travel' 'semiconductor' 'social' 'legal' 'transportation'\n",
      " 'hospitality' 'sports' 'nonprofit' 'fashion' 'messaging' 'music'\n",
      " 'automotive' 'design' 'real_estate' 'local' 'nanotech' 'pets']\n",
      "\n",
      "Unique values for 'country_code':\n",
      "['USA' 'Unknown' 'MAR' 'IND' 'AUS' 'FRA' 'JPN' 'NLD' 'EGY' 'ISR' 'GBR'\n",
      " 'THA' 'CAN' 'AUT' 'IRL' 'SWE' 'DEU' 'BRA' 'FIN' 'RUS' 'SGP' 'MEX' 'CHN'\n",
      " 'ESP' 'ISL' 'KOR' 'TUR' 'DNK' 'ARG' 'PAK' 'HUN' 'POL' 'GRC' 'PRT' 'BLR'\n",
      " 'CSS' 'MKD' 'CHE' 'SVN' 'UKR' 'ITA' 'NZL' 'LIE' 'NOR' 'CZE' 'VNM' 'HRV'\n",
      " 'BEN' 'CHL' 'GHA']\n",
      "... and 126 more.\n",
      "\n",
      "Unique values for 'state_code':\n",
      "['WA' 'CA' 'Unknown' 'NY' 'NM' 'TX' 'OH' 'NJ' 'IL' 'MA' 'NC' 'CT' 'PA'\n",
      " 'NH' 'MI' 'AZ' 'TN' 'GA' 'FL' 'MD' 'UT' 'OR' 'MO' 'AR' 'CO' 'KS' 'MN'\n",
      " 'NV' 'ID' 'IN' 'IA' 'VA' 'KY' 'LA' 'HI' 'WV' 'WI' 'SC' 'NE' 'DE' 'DC'\n",
      " 'AL' 'VT' 'RI' 'OK' 'ME' 'MS' 'MT' 'SD' 'ND']\n",
      "... and 2 more.\n",
      "\n",
      "Unique values for 'region':\n",
      "['Seattle' 'Los Angeles' 'SF Bay' 'unknown' 'Agadir' 'Vadodara' 'New York'\n",
      " 'Santa Fe' 'San Diego' 'Austin' 'Abbotsford' 'New Delhi' 'Columbus'\n",
      " 'New Jersey - Other' 'Chicago' 'Langhrone Creek' 'West Bridgewater'\n",
      " 'Houston' 'Charlotte' 'Paris' 'Shinagawa-ku' 'Amsterdam' 'Wilton'\n",
      " 'Philadelphia' 'Boston' 'Bangalore' 'Cairo' 'Tel Aviv' 'Portsmouth'\n",
      " 'Detroit' 'Iselin' 'London' 'Niantic' 'Amherst' 'Bangkok' 'Phoenix'\n",
      " 'New York - Other' 'Gatineau' 'Vienna' 'Mansfield' 'Cork' 'Nashville'\n",
      " 'Atlanta' 'Hattem' 'Liverpool' 'Leeds' 'Jacksonville' 'TBD' 'Kingston'\n",
      " 'Fort Lauderdale']\n",
      "... and 5797 more.\n",
      "\n",
      "Unique values for 'status':\n",
      "['operating' 'acquired' 'closed' 'ipo']\n",
      "\n",
      "Unique values for 'age_group':\n",
      "['15+y', '8-15y', NaN]\n",
      "Categories (4, object): ['0-3y' < '4-7y' < '8-15y' < '15+y']\n",
      "\n",
      "Unique values for 'risk_tier_label':\n",
      "['low_risk' 'high_risk' 'medium_risk']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "print(\"Categorical columns identified:\")\n",
    "print(categorical_cols)\n",
    "\n",
    "# Inspect unique values for potential inconsistencies in a few key categorical columns\n",
    "cols_to_inspect = ['category_code', 'country_code', 'state_code', 'region', 'status', 'age_group', 'risk_tier_label']\n",
    "\n",
    "for col in cols_to_inspect:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nUnique values for '{col}':\")\n",
    "        # Display a limited number of unique values if there are many\n",
    "        unique_values = df[col].unique()\n",
    "        if len(unique_values) > 50:\n",
    "            print(unique_values[:50])\n",
    "            print(f\"... and {len(unique_values) - 50} more.\")\n",
    "        else:\n",
    "            print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f92b4622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized 'category_code'.\n",
      "Standardized 'country_code'.\n",
      "Standardized 'state_code'.\n",
      "Standardized 'region'.\n",
      "Standardized 'status'.\n",
      "Standardized 'risk_tier_label'.\n",
      "\n",
      "Unique values for 'category_code' after standardization:\n",
      "['web' 'games_video' 'network_hosting' 'advertising' 'cleantech' 'other'\n",
      " 'enterprise' 'consulting' 'mobile' 'health' 'software' 'analytics'\n",
      " 'finance' 'education' 'medical' 'manufacturing' 'biotech' 'ecommerce'\n",
      " 'public_relations' 'hardware' 'search' 'news' 'government' 'security'\n",
      " 'photo_video' 'travel' 'semiconductor' 'social' 'legal' 'transportation'\n",
      " 'hospitality' 'sports' 'nonprofit' 'fashion' 'messaging' 'music'\n",
      " 'automotive' 'design' 'real_estate' 'local' 'nanotech' 'pets']\n",
      "\n",
      "Unique values for 'country_code' after standardization:\n",
      "['usa' 'unknown' 'mar' 'ind' 'aus' 'fra' 'jpn' 'nld' 'egy' 'isr' 'gbr'\n",
      " 'tha' 'can' 'aut' 'irl' 'swe' 'deu' 'bra' 'fin' 'rus' 'sgp' 'mex' 'chn'\n",
      " 'esp' 'isl' 'kor' 'tur' 'dnk' 'arg' 'pak' 'hun' 'pol' 'grc' 'prt' 'blr'\n",
      " 'css' 'mkd' 'che' 'svn' 'ukr' 'ita' 'nzl' 'lie' 'nor' 'cze' 'vnm' 'hrv'\n",
      " 'ben' 'chl' 'gha']\n",
      "... and 126 more.\n",
      "\n",
      "Unique values for 'state_code' after standardization:\n",
      "['wa' 'ca' 'unknown' 'ny' 'nm' 'tx' 'oh' 'nj' 'il' 'ma' 'nc' 'ct' 'pa'\n",
      " 'nh' 'mi' 'az' 'tn' 'ga' 'fl' 'md' 'ut' 'or' 'mo' 'ar' 'co' 'ks' 'mn'\n",
      " 'nv' 'id' 'in' 'ia' 'va' 'ky' 'la' 'hi' 'wv' 'wi' 'sc' 'ne' 'de' 'dc'\n",
      " 'al' 'vt' 'ri' 'ok' 'me' 'ms' 'mt' 'sd' 'nd']\n",
      "... and 2 more.\n",
      "\n",
      "Unique values for 'region' after standardization:\n",
      "['seattle' 'los angeles' 'sf bay' 'unknown' 'agadir' 'vadodara' 'new york'\n",
      " 'santa fe' 'san diego' 'austin' 'abbotsford' 'new delhi' 'columbus'\n",
      " 'new jersey - other' 'chicago' 'langhrone creek' 'west bridgewater'\n",
      " 'houston' 'charlotte' 'paris' 'shinagawa-ku' 'amsterdam' 'wilton'\n",
      " 'philadelphia' 'boston' 'bangalore' 'cairo' 'tel aviv' 'portsmouth'\n",
      " 'detroit' 'iselin' 'london' 'niantic' 'amherst' 'bangkok' 'phoenix'\n",
      " 'new york - other' 'gatineau' 'vienna' 'mansfield' 'cork' 'nashville'\n",
      " 'atlanta' 'hattem' 'liverpool' 'leeds' 'jacksonville' 'tbd' 'kingston'\n",
      " 'fort lauderdale']\n",
      "... and 5780 more.\n",
      "\n",
      "Unique values for 'status' after standardization:\n",
      "['operating' 'acquired' 'closed' 'ipo']\n",
      "\n",
      "Unique values for 'risk_tier_label' after standardization:\n",
      "['low_risk' 'high_risk' 'medium_risk']\n",
      "\n",
      "Unique values for 'age_group':\n",
      "['15+y', '8-15y', NaN]\n",
      "Categories (4, object): ['0-3y' < '4-7y' < '8-15y' < '15+y']\n"
     ]
    }
   ],
   "source": [
    "# Standardize categorical columns: convert to lowercase and strip whitespace\n",
    "cols_to_standardize = ['category_code', 'country_code', 'state_code', 'region', 'status', 'risk_tier_label']\n",
    "\n",
    "for col in cols_to_standardize:\n",
    "    if col in df.columns and df[col].dtype == 'object':\n",
    "        df[col] = df[col].str.lower().str.strip()\n",
    "        print(f\"Standardized '{col}'.\")\n",
    "\n",
    "# Re-verify unique values after standardization\n",
    "for col in cols_to_standardize:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nUnique values for '{col}' after standardization:\")\n",
    "        unique_values = df[col].unique()\n",
    "        if len(unique_values) > 50:\n",
    "            print(unique_values[:50])\n",
    "            print(f\"... and {len(unique_values) - 50} more.\")\n",
    "        else:\n",
    "            print(unique_values)\n",
    "\n",
    "# age_group is already a categorical type with defined labels, no string standardization needed.\n",
    "print(\"\\nUnique values for 'age_group':\")\n",
    "print(df['age_group'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6ac65433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows found: 0\n",
      "No duplicate rows to remove.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df.duplicated()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "\n",
    "print(f\"Number of duplicate rows found: {num_duplicates}\")\n",
    "\n",
    "# Remove duplicate rows if any exist\n",
    "if num_duplicates > 0:\n",
    "    rows_before_dropping = len(df)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    rows_after_dropping = len(df)\n",
    "    print(f\"Removed {rows_before_dropping - rows_after_dropping} duplicate rows.\")\n",
    "else:\n",
    "    print(\"No duplicate rows to remove.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Under-Capitalization Identification (CRITICAL MISSING)\n",
    "\n",
    "### Step 1: Calculate 2% Funding Threshold\n",
    "\n",
    "```\n",
    "ALGORITHM: Identify Target Population\n",
    "1. Load cleaned dataset from previous steps\n",
    "2. Calculate funding distribution percentiles [1%, 2%, 5%, 10%]\n",
    "3. Extract 2% percentile as under-capitalization threshold\n",
    "4. Display threshold value for validation\n",
    "\n",
    "EXPECTED OUTPUT:\n",
    "- Funding percentiles table\n",
    "- 2% threshold: \"$X,XXX\" (actual dollar amount)\n",
    "- Dataset coverage validation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Percentile Funding (USD)\n",
      "0         1%       $11,700\n",
      "1         2%       $19,616\n",
      "2         5%       $40,000\n",
      "3        10%      $100,000\n",
      "\n",
      "2% Threshold for funding_total_usd: $ 19616.0\n"
     ]
    }
   ],
   "source": [
    "percentiles = df['funding_total_usd'].quantile([0.01, 0.02, 0.05, 0.10])\n",
    "\n",
    "percentiles = percentiles.reset_index()\n",
    "percentiles.columns = ['Percentile', 'Funding (USD)']\n",
    "\n",
    "percentiles['Funding (USD)'] = percentiles['Funding (USD)'].apply(lambda x: f\"${x:,.0f}\")\n",
    "percentiles['Percentile'] = (percentiles['Percentile']*100).astype(int).astype(str) + '%'\n",
    "\n",
    "threshold_value = df['funding_total_usd'].quantile(0.02)\n",
    "\n",
    "print(percentiles)\n",
    "print(\"\\n2% Threshold for funding_total_usd: $\", threshold_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total companies in dataset: 196,530\n",
      "Companies with funding data: 27,873 (14.2%)\n",
      "Companies without funding: 168,657 (85.8%)\n",
      "\n",
      "Under-capitalized companies (≤ $19,616): 559\n",
      "Percentage of funded companies that are under-capitalized: 2.0%\n"
     ]
    }
   ],
   "source": [
    "total_companies = len(df)\n",
    "companies_with_funding = df['has_funding'].sum()\n",
    "companies_without_funding = total_companies - companies_with_funding\n",
    "\n",
    "print(f\"Total companies in dataset: {total_companies:,}\")\n",
    "print(f\"Companies with funding data: {companies_with_funding:,} ({companies_with_funding/total_companies:.1%})\")\n",
    "print(f\"Companies without funding: {companies_without_funding:,} ({companies_without_funding/total_companies:.1%})\")\n",
    "\n",
    "under_capitalized = (df['funding_total_usd'] <= threshold_value) & (df['has_funding'] == 1)\n",
    "under_cap_count = under_capitalized.sum()\n",
    "\n",
    "print(f\"\\nUnder-capitalized companies (≤ ${threshold_value:,.0f}): {under_cap_count:,}\")\n",
    "print(f\"Percentage of funded companies that are under-capitalized: {under_cap_count/companies_with_funding:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Under-Capitalized Flag\n",
    "\n",
    "```\n",
    "ALGORITHM: Under-Capitalized Population Flag\n",
    "1. Create boolean column 'under_capitalized' where:\n",
    "   - funding_total_usd <= 2% threshold OR\n",
    "   - funding_total_usd == 0 OR\n",
    "   - funding_total_usd is null\n",
    "2. Count total under-capitalized companies\n",
    "3. Calculate percentage of total dataset\n",
    "4. Validate against project requirement (~2% expectation)\n",
    "\n",
    "EXPECTED OUTPUT:\n",
    "- under_capitalized column added to dataframe\n",
    "- Population count: \"X,XXX companies (X.X%)\"\n",
    "- Validation: Should align with ~2% of VC funding recipients\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population count: 196,530 companies (86.1%)\n",
      "Validation: 2.0% of VC-funded companies are under-capitalized\n"
     ]
    }
   ],
   "source": [
    "df['under_capitalized'] = (\n",
    "    (df['funding_total_usd'] <= threshold_value) |\n",
    "    (df['funding_total_usd'] == 0) |\n",
    "    (df['funding_total_usd'].isna())\n",
    ")\n",
    "\n",
    "total_companies = len(df)\n",
    "under_cap_count = df['under_capitalized'].sum()\n",
    "under_cap_percentage = (under_cap_count / total_companies) * 100\n",
    "\n",
    "companies_with_funding = df['has_funding'].sum()\n",
    "funded_under_cap = ((df['funding_total_usd'] <= threshold_value) & (df['has_funding'] == 1)).sum()\n",
    "zero_funding = (df['funding_total_usd'] == 0).sum()\n",
    "null_funding = df['funding_total_usd'].isna().sum()\n",
    "\n",
    "print(f\"Population count: {total_companies:,} companies ({under_cap_percentage:.1f}%)\")\n",
    "\n",
    "if companies_with_funding > 0:\n",
    "    vc_under_cap_rate = (funded_under_cap / companies_with_funding) * 100\n",
    "    print(f\"Validation: {vc_under_cap_rate:.1f}% of VC-funded companies are under-capitalized\")\n",
    "\n",
    "# not 2% and instead 13% because the quantile measurement from pandas doesnt include the \n",
    "# null values for the threshold value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Population Validation\n",
    "\n",
    "```\n",
    "ALGORITHM: Under-Cap Population Characteristics\n",
    "1. Compare under-cap vs well-funded populations:\n",
    "   - Average company age\n",
    "   - Geographic distribution differences\n",
    "   - Industry sector patterns\n",
    "   - Success rate differentials\n",
    "2. Validate population makes business sense\n",
    "3. Document population characteristics for stakeholders\n",
    "\n",
    "EXPECTED OUTPUT:\n",
    "- Population comparison statistics\n",
    "- Business logic validation confirmed\n",
    "- Stakeholder-ready population summary\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison by Company Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   mean  median  std\n",
      "Well-Funded        17.5    16.0  5.7\n",
      "Under-Capitalized  15.4    13.0  6.1\n",
      "\n",
      "Age Distribution by Funding Status:\n",
      "           Well-Funded (%)  Under-Capitalized (%)\n",
      "age_group                                        \n",
      "8-15y                 33.7                   62.6\n",
      "15+y                  66.3                   37.4\n"
     ]
    }
   ],
   "source": [
    "age_comparison = df.groupby('under_capitalized')['company_age_years'].agg(['mean', 'median', 'std']).round(1)\n",
    "age_comparison.index = ['Well-Funded', 'Under-Capitalized']\n",
    "print(age_comparison)\n",
    "\n",
    "# Age distribution by funding status\n",
    "age_crosstab = pd.crosstab(df['age_group'], df['under_capitalized'], normalize='columns') * 100\n",
    "age_crosstab.columns = ['Well-Funded (%)', 'Under-Capitalized (%)']\n",
    "print(\"\\nAge Distribution by Funding Status:\")\n",
    "print(age_crosstab.round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Countries - Under-Capitalized vs Well-Funded:\n",
      "              Well-Funded (%)  Under-Capitalized (%)\n",
      "country_code                                        \n",
      "unknown                   5.4                   63.3\n",
      "usa                      65.6                   19.9\n",
      "gbr                       6.0                    3.4\n",
      "ind                       1.4                    2.1\n",
      "can                       3.1                    1.7\n",
      "deu                       1.6                    0.9\n",
      "aus                       0.7                    0.7\n",
      "fra                       2.3                    0.6\n",
      "irl                       0.7                    0.5\n",
      "esp                       1.2                    0.5\n",
      "\n",
      "Regional Distribution:\n",
      "               Well-Funded (%)  Under-Capitalized (%)\n",
      "region                                               \n",
      "unknown                    6.6                   63.9\n",
      "sf bay                    17.2                    3.2\n",
      "new york                   6.3                    2.0\n",
      "london                     3.3                    1.9\n",
      "los angeles                4.1                    1.7\n",
      "boston                     5.1                    0.8\n",
      "chicago                    1.4                    0.7\n",
      "washington dc              2.2                    0.7\n"
     ]
    }
   ],
   "source": [
    "# Top countries comparison\n",
    "print(\"Top 10 Countries - Under-Capitalized vs Well-Funded:\")\n",
    "geo_comparison = pd.crosstab(df['country_code'], df['under_capitalized'], normalize='columns') * 100\n",
    "geo_comparison.columns = ['Well-Funded (%)', 'Under-Capitalized (%)']\n",
    "geo_comparison = geo_comparison.sort_values('Under-Capitalized (%)', ascending=False).head(10)\n",
    "print(geo_comparison.round(1))\n",
    "\n",
    "# Regional distribution\n",
    "print(\"\\nRegional Distribution:\")\n",
    "region_comparison = pd.crosstab(df['region'], df['under_capitalized'], normalize='columns') * 100\n",
    "region_comparison.columns = ['Well-Funded (%)', 'Under-Capitalized (%)']\n",
    "region_comparison = region_comparison.sort_values('Under-Capitalized (%)', ascending=False).head(8)\n",
    "print(region_comparison.round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Industry Categories:\n",
      "                  Well-Funded (%)  Under-Capitalized (%)\n",
      "category_code                                           \n",
      "other                        5.48                  50.51\n",
      "software                    15.28                   8.12\n",
      "web                          8.29                   7.60\n",
      "ecommerce                    4.73                   4.59\n",
      "games_video                  4.18                   3.77\n",
      "mobile                       6.54                   3.00\n",
      "advertising                  3.88                   2.98\n",
      "consulting                   0.98                   2.80\n",
      "enterprise                   5.18                   1.79\n",
      "public_relations             1.29                   1.47\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 Industry Categories:\")\n",
    "industry_comparison = pd.crosstab(df['category_code'], df['under_capitalized'], normalize='columns') * 100\n",
    "industry_comparison.columns = ['Well-Funded (%)', 'Under-Capitalized (%)']\n",
    "industry_comparison = industry_comparison.sort_values('Under-Capitalized (%)', ascending=False).head(10)\n",
    "print(industry_comparison.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Tier Distribution by Funding Status:\n",
      "                 Well-Funded (%)  Under-Capitalized (%)\n",
      "risk_tier_label                                        \n",
      "high_risk                   6.04                  99.80\n",
      "low_risk                   73.09                   0.01\n",
      "medium_risk                20.87                   0.19\n",
      "\n",
      "Company Status Distribution:\n",
      "           Well-Funded (%)  Under-Capitalized (%)\n",
      "status                                           \n",
      "acquired              8.49                   4.18\n",
      "closed                6.04                   0.55\n",
      "ipo                   1.75                   0.39\n",
      "operating            83.72                  94.88\n",
      "\n",
      "Failure Risk Rates:\n",
      "Well-Funded Companies: 6.0% failure risk\n",
      "Under-Capitalized Companies: 99.8% failure risk\n"
     ]
    }
   ],
   "source": [
    "# Risk tier distribution\n",
    "print(\"Risk Tier Distribution by Funding Status:\")\n",
    "risk_comparison = pd.crosstab(df['risk_tier_label'], df['under_capitalized'], normalize='columns') * 100\n",
    "risk_comparison.columns = ['Well-Funded (%)', 'Under-Capitalized (%)']\n",
    "print(risk_comparison.round(2))\n",
    "\n",
    "# Company status distribution\n",
    "print(\"\\nCompany Status Distribution:\")\n",
    "status_comparison = pd.crosstab(df['status'], df['under_capitalized'], normalize='columns') * 100\n",
    "status_comparison.columns = ['Well-Funded (%)', 'Under-Capitalized (%)']\n",
    "print(status_comparison.round(2))\n",
    "\n",
    "# Failure risk rates\n",
    "print(\"\\nFailure Risk Rates:\")\n",
    "failure_rates = df.groupby('under_capitalized')['failure_risk'].mean() * 100\n",
    "failure_rates.index = ['Well-Funded', 'Under-Capitalized']\n",
    "print(f\"Well-Funded Companies: {failure_rates['Well-Funded']:.1f}% failure risk\")\n",
    "print(f\"Under-Capitalized Companies: {failure_rates['Under-Capitalized']:.1f}% failure risk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Companies: 196,530\n",
      "Under-Capitalized: 169,216 (86.1%)\n",
      "Well-Funded: 27,314 (13.9%)\n",
      "\n",
      "Key Differentials:\n",
      "Age Difference: Under-cap companies are -2.1 years older on average\n",
      "Failure Risk Difference: Under-cap companies have 93.8% higher failure risk\n"
     ]
    }
   ],
   "source": [
    "# Population sizes\n",
    "well_funded_count = (~df['under_capitalized']).sum()\n",
    "under_cap_count = df['under_capitalized'].sum()\n",
    "total_count = len(df)\n",
    "\n",
    "print(f\"Total Companies: {total_count:,}\")\n",
    "print(f\"Under-Capitalized: {under_cap_count:,} ({under_cap_count/total_count:.1%})\")\n",
    "print(f\"Well-Funded: {well_funded_count:,} ({well_funded_count/total_count:.1%})\")\n",
    "\n",
    "# Key validation metrics\n",
    "avg_age_diff = df[df['under_capitalized']]['company_age_years'].mean() - df[~df['under_capitalized']]['company_age_years'].mean()\n",
    "failure_risk_diff = df[df['under_capitalized']]['failure_risk'].mean() - df[~df['under_capitalized']]['failure_risk'].mean()\n",
    "\n",
    "print(f\"\\nKey Differentials:\")\n",
    "print(f\"Age Difference: Under-cap companies are {avg_age_diff:.1f} years older on average\")\n",
    "print(f\"Failure Risk Difference: Under-cap companies have {failure_risk_diff:.1%} higher failure risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNDER-CAPITALIZED COMPANY CHARACTERISTICS:\n",
      "• Population Size: 169,216 companies (86.1% of dataset)\n",
      "• Average Age: 15.4 years\n",
      "• Failure Risk: 99.8%\n",
      "• Geographic Concentration: Top 3 countries represent 86.6% of population\n",
      "• High-Risk Companies: 168,881 (99.8%)\n"
     ]
    }
   ],
   "source": [
    "print(\"UNDER-CAPITALIZED COMPANY CHARACTERISTICS:\")\n",
    "print(f\"• Population Size: {under_cap_count:,} companies ({under_cap_count/total_count:.1%} of dataset)\")\n",
    "print(f\"• Average Age: {df[df['under_capitalized']]['company_age_years'].mean():.1f} years\")\n",
    "print(f\"• Failure Risk: {df[df['under_capitalized']]['failure_risk'].mean():.1%}\")\n",
    "print(f\"• Geographic Concentration: Top 3 countries represent {geo_comparison.head(3)['Under-Capitalized (%)'].sum():.1f}% of population\")\n",
    "print(f\"• High-Risk Companies: {(df[df['under_capitalized']]['risk_tier'] == 2).sum():,} ({(df[df['under_capitalized']]['risk_tier'] == 2).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TYPICAL UNDER-CAPITALIZED COMPANY PROFILE:\n",
      "• Most Common Country: UNKNOWN\n",
      "• Most Common Industry: other\n",
      "• Most Common Age Group: 8-15y\n",
      "• Status: operating companies most common\n"
     ]
    }
   ],
   "source": [
    "# Top characteristics of under-capitalized companies\n",
    "top_country = geo_comparison.index[0]\n",
    "top_industry = industry_comparison.index[0]\n",
    "top_age_group = age_crosstab.idxmax(axis=0)['Under-Capitalized (%)']\n",
    "\n",
    "print(f\"\\nTYPICAL UNDER-CAPITALIZED COMPANY PROFILE:\")\n",
    "print(f\"• Most Common Country: {top_country.upper()}\")\n",
    "print(f\"• Most Common Industry: {top_industry}\")\n",
    "print(f\"• Most Common Age Group: {top_age_group}\")\n",
    "print(f\"• Status: {status_comparison.idxmax(axis=0)['Under-Capitalized (%)']} companies most common\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Geographic Standardization for Heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Geographic Data Validation\n",
    "\n",
    "```\n",
    "ALGORITHM: Geographic Coverage Assessment\n",
    "1. Analyze country_code distribution (focus on 'usa')\n",
    "2. Calculate US vs international company percentages\n",
    "3. Validate US market focus for project requirements\n",
    "4. Assess data completeness for heatmap requirements\n",
    "\n",
    "EXPECTED OUTPUT:\n",
    "- Country distribution summary\n",
    "- US market percentage: \"XX.X% US companies\"\n",
    "- International market assessment\n",
    "- Heatmap readiness evaluation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== USA ===\n",
      "Total Companies: 196,530\n",
      "US Companies: 51,635\n",
      "US Market Percentage: 26.3% US companies\n",
      "\n",
      "=== INTERNATIONAL ===\n",
      "International Companies: 36,345 (18.5%)\n",
      "Unknown Location: 108,550 (55.2%)\n",
      "\n",
      "Top 5 International Markets:\n",
      "  GBR: 7,372 companies (20.3% of international, 3.8% of total)\n",
      "  IND: 3,924 companies (10.8% of international, 2.0% of total)\n",
      "  CAN: 3,728 companies (10.3% of international, 1.9% of total)\n",
      "  DEU: 1,918 companies (5.3% of international, 1.0% of total)\n",
      "  FRA: 1,652 companies (4.5% of international, 0.8% of total)\n",
      "US State Code Coverage: 98.1% of US companies have valid state codes\n",
      "US Coordinate Coverage: 97.2% of US companies have lat/lng data\n",
      "\n",
      "=== STAKEHOLDER SUMMARY ===\n",
      "• Dataset Focus: 26.3% US-based companies\n",
      "• Geographic Coverage: 98.1% state-level, 97.2% coordinate-level\n",
      "• International Presence: 18.5% from 174 countries\n",
      "• Heatmap Capability: State-level ready\n"
     ]
    }
   ],
   "source": [
    "usa = df['country_code'] == 'usa'\n",
    "intl = (df['country_code'] != 'usa' ) & (df['country_code'] != 'unknown')\n",
    "\n",
    "us_companies = len(df[usa])\n",
    "total_companies = len(df)\n",
    "us_percentage = (us_companies / total_companies) * 100\n",
    "\n",
    "print(\"=== USA ===\")\n",
    "print(f\"Total Companies: {total_companies:,}\")\n",
    "print(f\"US Companies: {us_companies:,}\")\n",
    "print(f\"US Market Percentage: {us_percentage:.1f}% US companies\")\n",
    "\n",
    "# International Market Assessment\n",
    "print(\"\\n=== INTERNATIONAL ===\")\n",
    "intl_companies = len(df[intl])\n",
    "intl_percentage = (intl_companies / total_companies) * 100\n",
    "\n",
    "print(f\"International Companies: {intl_companies:,} ({intl_percentage:.1f}%)\")\n",
    "print(f\"Unknown Location: {len(df[~(usa|intl)]):,} ({len(df[~(usa|intl)])/total_companies:.1%})\")\n",
    "\n",
    "# Top international markets\n",
    "if intl_companies > 0:\n",
    "    print(\"\\nTop 5 International Markets:\")\n",
    "    intl_markets = df[intl]['country_code'].value_counts().head(5)\n",
    "    for country, count in intl_markets.items():\n",
    "        percentage = (count / intl_companies) * 100\n",
    "        global_pct = (count / total_companies) * 100\n",
    "        print(f\"  {country.upper()}: {count:,} companies ({percentage:.1f}% of international, {global_pct:.1f}% of total)\")\n",
    "\n",
    "# US State data completeness\n",
    "us_df = df[usa]\n",
    "us_with_state = us_df['state_code'].notna() & (us_df['state_code'] != 'unknown')\n",
    "us_state_coverage = (us_with_state.sum() / len(us_df)) * 100\n",
    "\n",
    "print(f\"US State Code Coverage: {us_state_coverage:.1f}% of US companies have valid state codes\")\n",
    "\n",
    "# Geographic coordinate coverage\n",
    "us_with_coords = us_df['lat'].notna() & us_df['lng'].notna()\n",
    "us_coord_coverage = (us_with_coords.sum() / len(us_df)) * 100\n",
    "\n",
    "print(f\"US Coordinate Coverage: {us_coord_coverage:.1f}% of US companies have lat/lng data\")\n",
    "\n",
    "# Data completeness summary for stakeholders\n",
    "print(f\"\\n=== STAKEHOLDER SUMMARY ===\")\n",
    "print(f\"• Dataset Focus: {us_percentage:.1f}% US-based companies\")\n",
    "print(f\"• Geographic Coverage: {us_state_coverage:.1f}% state-level, {us_coord_coverage:.1f}% coordinate-level\")\n",
    "print(f\"• International Presence: {intl_percentage:.1f}% from {len(df[intl]['country_code'].unique())} countries\")\n",
    "print(f\"• Heatmap Capability: {'State-level ready' if us_state_coverage >= 80 else 'State-level limited'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: US State Code Standardization\n",
    "\n",
    "```\n",
    "ALGORITHM: State Code Cleaning and Standardization\n",
    "1. Filter to US companies (country_code == 'usa')\n",
    "2. Identify non-standard state codes:\n",
    "   - Full state names vs abbreviations\n",
    "   - Inconsistent casing/formatting\n",
    "   - Invalid or missing state codes\n",
    "3. Create state name → abbreviation mapping dictionary\n",
    "4. Apply standardization transformations\n",
    "5. Validate all US state codes are 2-character format\n",
    "\n",
    "EXPECTED OUTPUT:\n",
    "- Standardized state_code column (all 2-char format)\n",
    "- State standardization report\n",
    "- Missing state data percentage\n",
    "- Heatmap-ready geographic data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US Companies without state codes: 0\n",
      "State codes longer than 2 characters: ['unknown']\n",
      "US companies with missing state codes: 0.0%\n"
     ]
    }
   ],
   "source": [
    "df_us = df[usa].copy()\n",
    "print(\"US Companies without state codes:\", df_us['state_code'].isna().sum())    # all companies in USA have state codes\n",
    "long_codes = df_us['state_code'].apply(lambda x: x if len(x) > 2 else None).dropna()\n",
    "print(\"State codes longer than 2 characters:\", long_codes.unique().tolist())\n",
    "\n",
    "# unknown -> uk, following standard 2 letter state code conventions ( idk if necessary but yeah )\n",
    "df_us['state_code'] = df_us['state_code'].apply(lambda x: 'uk' if x == 'unknown' else x)\n",
    "\n",
    "missing_states = df_us['state_code'].isna().sum()\n",
    "print(f\"US companies with missing state codes: {missing_states:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_map = {\n",
    "    'wa': 'Washington', 'or': 'Oregon', 'ca': 'California', 'nv': 'Nevada', 'id': 'Idaho',\n",
    "    'ut': 'Utah', 'az': 'Arizona', 'co': 'Colorado', 'nm': 'New Mexico', 'tx': 'Texas', 'wa': 'Washington',\n",
    "    'mt': 'Montana', 'wy': 'Wyoming', 'nd': 'North Dakota', 'sd': 'South Dakota', 'ne': 'Nebraska',\n",
    "    'ks': 'Kansas', 'ok': 'Oklahoma', 'mn': 'Minnesota', 'ia': 'Iowa', 'mo': 'Missouri', 'ar': 'Arkansas',\n",
    "    'la': 'Louisiana', 'wi': 'Wisconsin', 'il': 'Illinois', 'ms': 'Mississippi', 'mi': 'Michigan', 'in': 'Indiana',\n",
    "    'oh': 'Ohio', 'ky': 'Kentucky', 'tn': 'Tennessee', 'al': 'Alabama', 'fl': 'Florida', 'ga': 'Georgia', 'sc': 'South Carolina',\n",
    "    'nc': 'North Carolina', 'va': 'Virginia', 'wv': 'West Virginia', 'pa': 'Pennsylvania', 'md': 'Maryland', 'de': 'Delaware', \n",
    "    'nj': 'New Jersey', 'ny': 'New York', 'ct': 'Connecticut', 'ri': 'Rhode Island', 'ma': 'Massachusetts', 'vt': 'Vermont',\n",
    "    'nh': 'New Hampshire', 'me': 'Maine', 'ak': 'Alaska', 'hi': 'Hawaii'\n",
    "}\n",
    "\n",
    "df_us['state_name'] = df_us['state_code'].map(states_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Geographic Heatmap Data Preparation\n",
    "\n",
    "```\n",
    "ALGORITHM: Heatmap Data Structure Creation\n",
    "1. Create geographic aggregation columns:\n",
    "   - State-level startup counts\n",
    "   - State-level success rates\n",
    "   - State-level under-cap concentrations\n",
    "2. Validate geographic coverage completeness\n",
    "3. Prepare data structure for Month 3 dashboard\n",
    "4. Document geographic data limitations\n",
    "\n",
    "EXPECTED OUTPUT:\n",
    "- Geographic aggregation features ready\n",
    "- Heatmap data validation report\n",
    "- Dashboard-ready geographic dataset\n",
    "- Coverage limitation documentation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../processed_data/companies_cleaned_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
