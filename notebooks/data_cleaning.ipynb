{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "QZ98J4Q5eNi1",
    "outputId": "4d07485a-7db0-43a5-8394-d6d16b3db24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING OPTIMIZED TARGETS ===\n",
      "Total companies: 196,553\n",
      "\n",
      "=== OPTIMIZED TARGET DISTRIBUTION ===\n",
      "risk_tier_label\n",
      "high_risk      170553\n",
      "low_risk        19976\n",
      "medium_risk      6024\n",
      "Name: count, dtype: int64\n",
      "High risk rate: 86.8%\n",
      "\n",
      "=== VALIDATION BY STATUS ===\n",
      "risk_tier_label  high_risk  low_risk  medium_risk\n",
      "status                                           \n",
      "acquired          0.751756  0.248244     0.000000\n",
      "closed            1.000000  0.000000     0.000000\n",
      "ipo               0.577601  0.422399     0.000000\n",
      "operating         0.873589  0.093572     0.032839\n",
      "\n",
      "=== RISK BY AGE GROUPS ===\n",
      "risk_tier_label  high_risk  low_risk  medium_risk\n",
      "age_group                                        \n",
      "8-15y             0.919854  0.049031     0.031114\n",
      "15+y              0.792937  0.176687     0.030376\n",
      "\n",
      "Saved optimized targets to processed_data/companies_optimized_targets.csv\n",
      "Final dataset: 196,553 companies with 16 columns\n",
      "\n",
      "==================================================\n",
      "RISK TIER CHARACTERISTICS\n",
      "==================================================\n",
      "\n",
      "--- TIER 0 (low_risk) ---\n",
      "Count: 19,976 companies\n",
      "Avg age: 18.2 years\n",
      "Avg funding: $19,887,642\n",
      "Has funding: 100.0%\n",
      "Top statuses:\n",
      "status\n",
      "operating    17165\n",
      "acquired      2332\n",
      "ipo            479\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- TIER 1 (medium_risk) ---\n",
      "Count: 6,024 companies\n",
      "Avg age: 15.0 years\n",
      "Avg funding: $196,702\n",
      "Has funding: 100.0%\n",
      "Top statuses:\n",
      "status\n",
      "operating    6024\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- TIER 2 (high_risk) ---\n",
      "Count: 170,553 companies\n",
      "Avg age: 15.4 years\n",
      "Avg funding: $7,756,252\n",
      "Has funding: 1.1%\n",
      "Top statuses:\n",
      "status\n",
      "operating    160252\n",
      "acquired       7062\n",
      "closed         2584\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_optimized_targets():\n",
    "    \"\"\"\n",
    "    Create final optimized target variables with better balance\n",
    "    \"\"\"\n",
    "    # Load the original data (not the one with targets)\n",
    "    df = pd.read_csv('../data/companies.csv')\n",
    "\n",
    "    # Basic preprocessing (from your original script)\n",
    "    df['founded_at'] = pd.to_datetime(df['founded_at'], errors='coerce')\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "\n",
    "    current_year = pd.Timestamp.now().year\n",
    "    df['founded_year'] = df['founded_at'].dt.year\n",
    "    df['company_age_years'] = current_year - df['founded_year']\n",
    "\n",
    "    # Handle missing founded years\n",
    "    missing_founded = df['founded_year'].isna()\n",
    "    df.loc[missing_founded, 'founded_year'] = df.loc[missing_founded, 'created_at'].dt.year\n",
    "    df.loc[missing_founded, 'company_age_years'] = current_year - df.loc[missing_founded, 'founded_year']\n",
    "    df['company_age_years'] = df['company_age_years'].clip(upper=50, lower=0)\n",
    "\n",
    "    # Funding features\n",
    "    df['funding_total_usd'] = pd.to_numeric(df['funding_total_usd'], errors='coerce')\n",
    "    df['has_funding'] = (~df['funding_total_usd'].isna() & (df['funding_total_usd'] > 0)).astype(int)\n",
    "\n",
    "    print(\"=== CREATING OPTIMIZED TARGETS ===\")\n",
    "    print(f\"Total companies: {len(df):,}\")\n",
    "\n",
    "    # Initialize targets\n",
    "    df['failure_risk'] = 0\n",
    "    df['risk_tier'] = 1  # 0=Low, 1=Medium, 2=High\n",
    "\n",
    "    # TIER 0: LOW RISK (Successful/Strong Companies)\n",
    "    low_risk_conditions = [\n",
    "        # Successful exits - ALWAYS low risk\n",
    "        df['status'].isin(['ipo', 'acquired']),\n",
    "\n",
    "        # Well-funded operating companies\n",
    "        (df['status'] == 'operating') & (df['funding_total_usd'] > 500000),\n",
    "\n",
    "        # Young companies with decent funding\n",
    "        (df['company_age_years'] <= 3) & (df['funding_total_usd'] > 100000),\n",
    "\n",
    "        # Companies with significant funding regardless of age\n",
    "        (df['funding_total_usd'] > 2000000)\n",
    "    ]\n",
    "\n",
    "    low_risk_mask = pd.concat(low_risk_conditions, axis=1).any(axis=1)\n",
    "    df.loc[low_risk_mask, 'risk_tier'] = 0\n",
    "    df.loc[low_risk_mask, 'failure_risk'] = 0\n",
    "\n",
    "    # TIER 2: HIGH RISK (Clear Failure Signals)\n",
    "    high_risk_conditions = [\n",
    "        # Explicitly closed\n",
    "        df['status'] == 'closed',\n",
    "\n",
    "        # Very old with no funding (true zombies)\n",
    "        (df['company_age_years'] > 10) & (df['has_funding'] == 0),\n",
    "\n",
    "        # Old with extremely low funding\n",
    "        (df['company_age_years'] > 8) & (df['funding_total_usd'] < 10000),\n",
    "    ]\n",
    "\n",
    "    high_risk_mask = pd.concat(high_risk_conditions, axis=1).any(axis=1)\n",
    "    df.loc[high_risk_mask, 'risk_tier'] = 2\n",
    "    df.loc[high_risk_mask, 'failure_risk'] = 1\n",
    "\n",
    "    # TIER 1: MEDIUM RISK (Everything else - the uncertain middle)\n",
    "    # This is automatic based on the initialization\n",
    "\n",
    "    # Map to readable labels\n",
    "    risk_labels = {0: 'low_risk', 1: 'medium_risk', 2: 'high_risk'}\n",
    "    df['risk_tier_label'] = df['risk_tier'].map(risk_labels)\n",
    "\n",
    "    # Validation\n",
    "    print(\"\\n=== OPTIMIZED TARGET DISTRIBUTION ===\")\n",
    "    print(df['risk_tier_label'].value_counts().sort_index())\n",
    "    print(f\"High risk rate: {df['failure_risk'].mean():.1%}\")\n",
    "\n",
    "    # Validate key segments\n",
    "    print(\"\\n=== VALIDATION BY STATUS ===\")\n",
    "    status_risk = pd.crosstab(df['status'], df['risk_tier_label'], normalize='index')\n",
    "    print(status_risk)\n",
    "\n",
    "    # Analyze risk by age and funding\n",
    "    print(\"\\n=== RISK BY AGE GROUPS ===\")\n",
    "    df['age_group'] = pd.cut(df['company_age_years'],\n",
    "                            bins=[0, 3, 7, 15, 50],\n",
    "                            labels=['0-3y', '4-7y', '8-15y', '15+y'],\n",
    "                            right=False)\n",
    "    age_risk = pd.crosstab(df['age_group'], df['risk_tier_label'], normalize='index')\n",
    "    print(age_risk)\n",
    "\n",
    "    # Save optimized version\n",
    "    output_columns = [\n",
    "        'id', 'name', 'status', 'category_code', 'country_code', 'state_code', 'region',\n",
    "        'founded_at', 'founded_year', 'company_age_years', 'age_group',\n",
    "        'funding_total_usd', 'has_funding',\n",
    "        'failure_risk', 'risk_tier', 'risk_tier_label'\n",
    "    ]\n",
    "\n",
    "    # Only include existing columns\n",
    "    existing_columns = [col for col in output_columns if col in df.columns]\n",
    "    df_output = df[existing_columns]\n",
    "\n",
    "    df_output.to_csv('../processed_data/companies_optimized_targets.csv', index=False)\n",
    "    print(f\"\\nSaved optimized targets to processed_data/companies_optimized_targets.csv\")\n",
    "    print(f\"Final dataset: {len(df_output):,} companies with {len(existing_columns)} columns\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def analyze_risk_characteristics(df):\n",
    "    \"\"\"\n",
    "    Analyze what characterizes each risk tier\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RISK TIER CHARACTERISTICS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for tier in [0, 1, 2]:\n",
    "        tier_data = df[df['risk_tier'] == tier]\n",
    "        print(f\"\\n--- TIER {tier} ({tier_data['risk_tier_label'].iloc[0]}) ---\")\n",
    "        print(f\"Count: {len(tier_data):,} companies\")\n",
    "        print(f\"Avg age: {tier_data['company_age_years'].mean():.1f} years\")\n",
    "        print(f\"Avg funding: ${tier_data['funding_total_usd'].mean():,.0f}\")\n",
    "        print(f\"Has funding: {tier_data['has_funding'].mean():.1%}\")\n",
    "        print(f\"Top statuses:\")\n",
    "        print(tier_data['status'].value_counts().head(3))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_optimized = create_optimized_targets()\n",
    "    analyze_risk_characteristics(df_optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc7f31c7"
   },
   "source": [
    "# Task\n",
    "Clean the dataset by handling missing values, standardizing categorical variables, and removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ababcbac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values per column:\n",
      "parent_id              100.000000\n",
      "ROI                     99.630634\n",
      "last_investment_at      98.685851\n",
      "first_investment_at     98.685851\n",
      "invested_companies      98.681780\n",
      "investment_rounds       98.681780\n",
      "closed_at               98.667026\n",
      "short_description       96.371971\n",
      "funding_total_usd       85.818583\n",
      "last_funding_at         83.970227\n",
      "first_funding_at        83.970227\n",
      "funding_rounds          83.868473\n",
      "state_code              74.102151\n",
      "twitter_username        58.997828\n",
      "tag_list                58.559778\n",
      "lng                     57.338733\n",
      "lat                     57.338733\n",
      "city                    57.319400\n",
      "country_code            55.233448\n",
      "founded_at              53.586564\n",
      "milestones              53.346426\n",
      "last_milestone_at       53.346426\n",
      "first_milestone_at      53.346426\n",
      "description             53.168865\n",
      "logo_height             43.979486\n",
      "logo_url                43.979486\n",
      "logo_width              43.979486\n",
      "category_code           37.326828\n",
      "domain                  35.617874\n",
      "homepage_url            35.617874\n",
      "overview                35.401139\n",
      "relationships           34.029498\n",
      "created_by              20.869689\n",
      "age_group                0.800802\n",
      "normalized_name          0.013228\n",
      "name                     0.011702\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing_percentages = df_optimized.isnull().mean() * 100\n",
    "missing_percentages = missing_percentages.sort_values(ascending=False)\n",
    "\n",
    "print(\"Percentage of missing values per column:\")\n",
    "print(missing_percentages[missing_percentages > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d849a3eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 24 columns with >50% missing values.\n",
      "Imputed missing values in 'category_code' with mode.\n",
      "Imputed missing values in 'region' with mode.\n",
      "Imputed missing values in 'age_group' with mode.\n",
      "Imputed missing values in 'relationships' with mode.\n",
      "Imputed missing values in 'created_by' with mode.\n",
      "Imputed missing values in 'overview' with mode.\n",
      "Imputed missing values in 'homepage_url' with mode.\n",
      "Imputed missing values in 'domain' with mode.\n",
      "Imputed missing values in 'logo_height' with median.\n",
      "Imputed missing values in 'logo_width' with median.\n",
      "Dropped 23 rows with missing values in 'name'.\n",
      "Dropped 4 rows with missing values in 'normalized_name'.\n",
      "\n",
      "Percentage of missing values per column after cleaning:\n",
      "logo_url    43.977896\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with missing percentage > 50%\n",
    "columns_to_drop = missing_percentages[missing_percentages > 50].index\n",
    "df_optimized = df_optimized.drop(columns=columns_to_drop)\n",
    "print(f\"Dropped {len(columns_to_drop)} columns with >50% missing values.\")\n",
    "\n",
    "moderate_missing_cols_categorical = ['category_code', 'country_code', 'state_code', 'region', 'age_group', 'city', 'twitter_username', 'tag_list', 'description', 'relationships', 'created_by', 'overview', 'homepage_url', 'domain']\n",
    "moderate_missing_cols_numerical = ['lat', 'lng', 'logo_height', 'logo_width']\n",
    "\n",
    "for col in moderate_missing_cols_categorical:\n",
    "    if col in df_optimized.columns:\n",
    "        mode_value = df_optimized[col].mode()[0]\n",
    "        df_optimized[col] = df_optimized[col].fillna(mode_value)\n",
    "        print(f\"Imputed missing values in '{col}' with mode.\")\n",
    "\n",
    "for col in moderate_missing_cols_numerical:\n",
    "    if col in df_optimized.columns:\n",
    "        median_value = df_optimized[col].median()\n",
    "        df_optimized[col] = df_optimized[col].fillna(median_value)\n",
    "        print(f\"Imputed missing values in '{col}' with median.\")\n",
    "\n",
    "# Drop rows for low missing percentages (< 20%)\n",
    "low_missing_cols = ['name', 'normalized_name']\n",
    "\n",
    "for col in low_missing_cols:\n",
    "    if col in df_optimized.columns:\n",
    "        rows_before = len(df_optimized)\n",
    "        df_optimized.dropna(subset=[col], inplace=True)\n",
    "        rows_after = len(df_optimized)\n",
    "        print(f\"Dropped {rows_before - rows_after} rows with missing values in '{col}'.\")\n",
    "\n",
    "\n",
    "# Verify remaining missing values\n",
    "missing_percentages_after = df_optimized.isnull().mean() * 100\n",
    "missing_percentages_after = missing_percentages_after.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nPercentage of missing values per column after cleaning:\")\n",
    "print(missing_percentages_after[missing_percentages_after > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2bc5f437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 86428 rows with missing values in 'logo_url'.\n",
      "\n",
      "Percentage of missing values per column after final cleaning:\n",
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "# Handle remaining columns with minimal missing values by dropping rows\n",
    "remaining_cols_with_missing = missing_percentages_after[missing_percentages_after > 0].index.tolist()\n",
    "\n",
    "for col in remaining_cols_with_missing:\n",
    "    if col in df_optimized.columns:\n",
    "        rows_before = len(df_optimized)\n",
    "        df_optimized.dropna(subset=[col], inplace=True)\n",
    "        rows_after = len(df_optimized)\n",
    "        print(f\"Dropped {rows_before - rows_after} rows with missing values in '{col}'.\")\n",
    "\n",
    "\n",
    "# Verify that there are no remaining missing values\n",
    "missing_percentages_final = df_optimized.isnull().mean() * 100\n",
    "\n",
    "print(\"\\nPercentage of missing values per column after final cleaning:\")\n",
    "print(missing_percentages_final[missing_percentages_final > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "845adab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns identified:\n",
      "Index(['id', 'entity_type', 'name', 'normalized_name', 'permalink',\n",
      "       'category_code', 'status', 'domain', 'homepage_url', 'logo_url',\n",
      "       'overview', 'region', 'created_by', 'updated_at', 'risk_tier_label'],\n",
      "      dtype='object')\n",
      "\n",
      "Unique values for 'category_code':\n",
      "['web' 'games_video' 'advertising' 'software' 'enterprise' 'other'\n",
      " 'consulting' 'mobile' 'health' 'cleantech' 'analytics' 'network_hosting'\n",
      " 'finance' 'education' 'medical' 'manufacturing' 'ecommerce'\n",
      " 'public_relations' 'hardware' 'search' 'news' 'security' 'biotech'\n",
      " 'photo_video' 'travel' 'social' 'legal' 'transportation' 'hospitality'\n",
      " 'sports' 'nonprofit' 'semiconductor' 'fashion' 'messaging' 'music'\n",
      " 'automotive' 'design' 'real_estate' 'local' 'nanotech' 'pets'\n",
      " 'government']\n",
      "\n",
      "Unique values for 'region':\n",
      "['Seattle' 'Los Angeles' 'SF Bay' 'unknown' 'Agadir' 'New York' 'Santa Fe'\n",
      " 'San Diego' 'Austin' 'Abbotsford' 'New Delhi' 'Columbus'\n",
      " 'New Jersey - Other' 'Chicago' 'West Bridgewater' 'Houston' 'Charlotte'\n",
      " 'Paris' 'Shinagawa-ku' 'Amsterdam' 'Wilton' 'Philadelphia' 'Boston'\n",
      " 'Bangalore' 'Cairo' 'Tel Aviv' 'Portsmouth' 'Detroit' 'Iselin' 'London'\n",
      " 'Niantic' 'Amherst' 'Bangkok' 'Phoenix' 'New York - Other' 'Gatineau'\n",
      " 'Vienna' 'Mansfield' 'Cork' 'Nashville' 'Atlanta' 'Hattem' 'Liverpool'\n",
      " 'Leeds' 'Jacksonville' 'TBD' 'Kingston' 'Fort Lauderdale'\n",
      " 'Maryland - Other' 'Salt Lake City']\n",
      "... and 5438 more.\n",
      "\n",
      "Unique values for 'status':\n",
      "['operating' 'acquired' 'closed' 'ipo']\n",
      "\n",
      "Unique values for 'age_group':\n",
      "['15+y', '8-15y']\n",
      "Categories (4, object): ['0-3y' < '4-7y' < '8-15y' < '15+y']\n",
      "\n",
      "Unique values for 'risk_tier_label':\n",
      "['low_risk' 'high_risk' 'medium_risk']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df_optimized.select_dtypes(include='object').columns\n",
    "print(\"Categorical columns identified:\")\n",
    "print(categorical_cols)\n",
    "\n",
    "# Inspect unique values for potential inconsistencies in a few key categorical columns\n",
    "cols_to_inspect = ['category_code', 'country_code', 'state_code', 'region', 'status', 'age_group', 'risk_tier_label']\n",
    "\n",
    "for col in cols_to_inspect:\n",
    "    if col in df_optimized.columns:\n",
    "        print(f\"\\nUnique values for '{col}':\")\n",
    "        # Display a limited number of unique values if there are many\n",
    "        unique_values = df_optimized[col].unique()\n",
    "        if len(unique_values) > 50:\n",
    "            print(unique_values[:50])\n",
    "            print(f\"... and {len(unique_values) - 50} more.\")\n",
    "        else:\n",
    "            print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f92b4622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized 'category_code'.\n",
      "Standardized 'region'.\n",
      "Standardized 'status'.\n",
      "Standardized 'risk_tier_label'.\n",
      "\n",
      "Unique values for 'category_code' after standardization:\n",
      "['web' 'games_video' 'advertising' 'software' 'enterprise' 'other'\n",
      " 'consulting' 'mobile' 'health' 'cleantech' 'analytics' 'network_hosting'\n",
      " 'finance' 'education' 'medical' 'manufacturing' 'ecommerce'\n",
      " 'public_relations' 'hardware' 'search' 'news' 'security' 'biotech'\n",
      " 'photo_video' 'travel' 'social' 'legal' 'transportation' 'hospitality'\n",
      " 'sports' 'nonprofit' 'semiconductor' 'fashion' 'messaging' 'music'\n",
      " 'automotive' 'design' 'real_estate' 'local' 'nanotech' 'pets'\n",
      " 'government']\n",
      "\n",
      "Unique values for 'region' after standardization:\n",
      "['seattle' 'los angeles' 'sf bay' 'unknown' 'agadir' 'new york' 'santa fe'\n",
      " 'san diego' 'austin' 'abbotsford' 'new delhi' 'columbus'\n",
      " 'new jersey - other' 'chicago' 'west bridgewater' 'houston' 'charlotte'\n",
      " 'paris' 'shinagawa-ku' 'amsterdam' 'wilton' 'philadelphia' 'boston'\n",
      " 'bangalore' 'cairo' 'tel aviv' 'portsmouth' 'detroit' 'iselin' 'london'\n",
      " 'niantic' 'amherst' 'bangkok' 'phoenix' 'new york - other' 'gatineau'\n",
      " 'vienna' 'mansfield' 'cork' 'nashville' 'atlanta' 'hattem' 'liverpool'\n",
      " 'leeds' 'jacksonville' 'tbd' 'kingston' 'fort lauderdale'\n",
      " 'maryland - other' 'salt lake city']\n",
      "... and 5422 more.\n",
      "\n",
      "Unique values for 'status' after standardization:\n",
      "['operating' 'acquired' 'closed' 'ipo']\n",
      "\n",
      "Unique values for 'risk_tier_label' after standardization:\n",
      "['low_risk' 'high_risk' 'medium_risk']\n",
      "\n",
      "Unique values for 'age_group':\n",
      "['15+y', '8-15y']\n",
      "Categories (4, object): ['0-3y' < '4-7y' < '8-15y' < '15+y']\n"
     ]
    }
   ],
   "source": [
    "# Standardize categorical columns: convert to lowercase and strip whitespace\n",
    "cols_to_standardize = ['category_code', 'country_code', 'state_code', 'region', 'status', 'risk_tier_label']\n",
    "\n",
    "for col in cols_to_standardize:\n",
    "    if col in df_optimized.columns and df_optimized[col].dtype == 'object':\n",
    "        df_optimized[col] = df_optimized[col].str.lower().str.strip()\n",
    "        print(f\"Standardized '{col}'.\")\n",
    "\n",
    "# Re-verify unique values after standardization\n",
    "for col in cols_to_standardize:\n",
    "    if col in df_optimized.columns:\n",
    "        print(f\"\\nUnique values for '{col}' after standardization:\")\n",
    "        unique_values = df_optimized[col].unique()\n",
    "        if len(unique_values) > 50:\n",
    "            print(unique_values[:50])\n",
    "            print(f\"... and {len(unique_values) - 50} more.\")\n",
    "        else:\n",
    "            print(unique_values)\n",
    "\n",
    "# age_group is already a categorical type with defined labels, no string standardization needed.\n",
    "print(\"\\nUnique values for 'age_group':\")\n",
    "print(df_optimized['age_group'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6ac65433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows found: 0\n",
      "No duplicate rows to remove.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df_optimized.duplicated()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "\n",
    "print(f\"Number of duplicate rows found: {num_duplicates}\")\n",
    "\n",
    "# Remove duplicate rows if any exist\n",
    "if num_duplicates > 0:\n",
    "    rows_before_dropping = len(df_optimized)\n",
    "    df_optimized.drop_duplicates(inplace=True)\n",
    "    rows_after_dropping = len(df_optimized)\n",
    "    print(f\"Removed {rows_before_dropping - rows_after_dropping} duplicate rows.\")\n",
    "else:\n",
    "    print(\"No duplicate rows to remove.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimized.to_csv('../processed_data/companies_cleaned_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
