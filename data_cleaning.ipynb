{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "QZ98J4Q5eNi1",
    "outputId": "4d07485a-7db0-43a5-8394-d6d16b3db24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING OPTIMIZED TARGETS ===\n",
      "Total companies: 196,553\n",
      "\n",
      "=== OPTIMIZED TARGET DISTRIBUTION ===\n",
      "risk_tier_label\n",
      "high_risk      170553\n",
      "low_risk        19976\n",
      "medium_risk      6024\n",
      "Name: count, dtype: int64\n",
      "High risk rate: 86.8%\n",
      "\n",
      "=== VALIDATION BY STATUS ===\n",
      "risk_tier_label  high_risk  low_risk  medium_risk\n",
      "status                                           \n",
      "acquired          0.751756  0.248244     0.000000\n",
      "closed            1.000000  0.000000     0.000000\n",
      "ipo               0.577601  0.422399     0.000000\n",
      "operating         0.873589  0.093572     0.032839\n",
      "\n",
      "=== RISK BY AGE GROUPS ===\n",
      "risk_tier_label  high_risk  low_risk  medium_risk\n",
      "age_group                                        \n",
      "8-15y             0.919854  0.049031     0.031114\n",
      "15+y              0.792937  0.176687     0.030376\n",
      "\n",
      "Saved optimized targets to companies_optimized_targets.csv\n",
      "Final dataset: 196,553 companies with 16 columns\n",
      "\n",
      "==================================================\n",
      "RISK TIER CHARACTERISTICS\n",
      "==================================================\n",
      "\n",
      "--- TIER 0 (low_risk) ---\n",
      "Count: 19,976 companies\n",
      "Avg age: 18.2 years\n",
      "Avg funding: $19,887,642\n",
      "Has funding: 100.0%\n",
      "Top statuses:\n",
      "status\n",
      "operating    17165\n",
      "acquired      2332\n",
      "ipo            479\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- TIER 1 (medium_risk) ---\n",
      "Count: 6,024 companies\n",
      "Avg age: 15.0 years\n",
      "Avg funding: $196,702\n",
      "Has funding: 100.0%\n",
      "Top statuses:\n",
      "status\n",
      "operating    6024\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- TIER 2 (high_risk) ---\n",
      "Count: 170,553 companies\n",
      "Avg age: 15.4 years\n",
      "Avg funding: $7,756,252\n",
      "Has funding: 1.1%\n",
      "Top statuses:\n",
      "status\n",
      "operating    160252\n",
      "acquired       7062\n",
      "closed         2584\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_optimized_targets():\n",
    "    \"\"\"\n",
    "    Create final optimized target variables with better balance\n",
    "    \"\"\"\n",
    "    # Load the original data (not the one with targets)\n",
    "    df = pd.read_csv('./data/companies.csv')\n",
    "\n",
    "    # Basic preprocessing (from your original script)\n",
    "    df['founded_at'] = pd.to_datetime(df['founded_at'], errors='coerce')\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "\n",
    "    current_year = pd.Timestamp.now().year\n",
    "    df['founded_year'] = df['founded_at'].dt.year\n",
    "    df['company_age_years'] = current_year - df['founded_year']\n",
    "\n",
    "    # Handle missing founded years\n",
    "    missing_founded = df['founded_year'].isna()\n",
    "    df.loc[missing_founded, 'founded_year'] = df.loc[missing_founded, 'created_at'].dt.year\n",
    "    df.loc[missing_founded, 'company_age_years'] = current_year - df.loc[missing_founded, 'founded_year']\n",
    "    df['company_age_years'] = df['company_age_years'].clip(upper=50, lower=0)\n",
    "\n",
    "    # Funding features\n",
    "    df['funding_total_usd'] = pd.to_numeric(df['funding_total_usd'], errors='coerce')\n",
    "    df['has_funding'] = (~df['funding_total_usd'].isna() & (df['funding_total_usd'] > 0)).astype(int)\n",
    "\n",
    "    print(\"=== CREATING OPTIMIZED TARGETS ===\")\n",
    "    print(f\"Total companies: {len(df):,}\")\n",
    "\n",
    "    # Initialize targets\n",
    "    df['failure_risk'] = 0\n",
    "    df['risk_tier'] = 1  # 0=Low, 1=Medium, 2=High\n",
    "\n",
    "    # TIER 0: LOW RISK (Successful/Strong Companies)\n",
    "    low_risk_conditions = [\n",
    "        # Successful exits - ALWAYS low risk\n",
    "        df['status'].isin(['ipo', 'acquired']),\n",
    "\n",
    "        # Well-funded operating companies\n",
    "        (df['status'] == 'operating') & (df['funding_total_usd'] > 500000),\n",
    "\n",
    "        # Young companies with decent funding\n",
    "        (df['company_age_years'] <= 3) & (df['funding_total_usd'] > 100000),\n",
    "\n",
    "        # Companies with significant funding regardless of age\n",
    "        (df['funding_total_usd'] > 2000000)\n",
    "    ]\n",
    "\n",
    "    low_risk_mask = pd.concat(low_risk_conditions, axis=1).any(axis=1)\n",
    "    df.loc[low_risk_mask, 'risk_tier'] = 0\n",
    "    df.loc[low_risk_mask, 'failure_risk'] = 0\n",
    "\n",
    "    # TIER 2: HIGH RISK (Clear Failure Signals)\n",
    "    high_risk_conditions = [\n",
    "        # Explicitly closed\n",
    "        df['status'] == 'closed',\n",
    "\n",
    "        # Very old with no funding (true zombies)\n",
    "        (df['company_age_years'] > 10) & (df['has_funding'] == 0),\n",
    "\n",
    "        # Old with extremely low funding\n",
    "        (df['company_age_years'] > 8) & (df['funding_total_usd'] < 10000),\n",
    "    ]\n",
    "\n",
    "    high_risk_mask = pd.concat(high_risk_conditions, axis=1).any(axis=1)\n",
    "    df.loc[high_risk_mask, 'risk_tier'] = 2\n",
    "    df.loc[high_risk_mask, 'failure_risk'] = 1\n",
    "\n",
    "    # TIER 1: MEDIUM RISK (Everything else - the uncertain middle)\n",
    "    # This is automatic based on the initialization\n",
    "\n",
    "    # Map to readable labels\n",
    "    risk_labels = {0: 'low_risk', 1: 'medium_risk', 2: 'high_risk'}\n",
    "    df['risk_tier_label'] = df['risk_tier'].map(risk_labels)\n",
    "\n",
    "    # Validation\n",
    "    print(\"\\n=== OPTIMIZED TARGET DISTRIBUTION ===\")\n",
    "    print(df['risk_tier_label'].value_counts().sort_index())\n",
    "    print(f\"High risk rate: {df['failure_risk'].mean():.1%}\")\n",
    "\n",
    "    # Validate key segments\n",
    "    print(\"\\n=== VALIDATION BY STATUS ===\")\n",
    "    status_risk = pd.crosstab(df['status'], df['risk_tier_label'], normalize='index')\n",
    "    print(status_risk)\n",
    "\n",
    "    # Analyze risk by age and funding\n",
    "    print(\"\\n=== RISK BY AGE GROUPS ===\")\n",
    "    df['age_group'] = pd.cut(df['company_age_years'],\n",
    "                            bins=[0, 3, 7, 15, 50],\n",
    "                            labels=['0-3y', '4-7y', '8-15y', '15+y'],\n",
    "                            right=False)\n",
    "    age_risk = pd.crosstab(df['age_group'], df['risk_tier_label'], normalize='index')\n",
    "    print(age_risk)\n",
    "\n",
    "    # Save optimized version\n",
    "    output_columns = [\n",
    "        'id', 'name', 'status', 'category_code', 'country_code', 'state_code', 'region',\n",
    "        'founded_at', 'founded_year', 'company_age_years', 'age_group',\n",
    "        'funding_total_usd', 'has_funding',\n",
    "        'failure_risk', 'risk_tier', 'risk_tier_label'\n",
    "    ]\n",
    "\n",
    "    # Only include existing columns\n",
    "    existing_columns = [col for col in output_columns if col in df.columns]\n",
    "    df_output = df[existing_columns]\n",
    "\n",
    "    df_output.to_csv('companies_optimized_targets.csv', index=False)\n",
    "    print(f\"\\nSaved optimized targets to companies_optimized_targets.csv\")\n",
    "    print(f\"Final dataset: {len(df_output):,} companies with {len(existing_columns)} columns\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def analyze_risk_characteristics(df):\n",
    "    \"\"\"\n",
    "    Analyze what characterizes each risk tier\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RISK TIER CHARACTERISTICS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for tier in [0, 1, 2]:\n",
    "        tier_data = df[df['risk_tier'] == tier]\n",
    "        print(f\"\\n--- TIER {tier} ({tier_data['risk_tier_label'].iloc[0]}) ---\")\n",
    "        print(f\"Count: {len(tier_data):,} companies\")\n",
    "        print(f\"Avg age: {tier_data['company_age_years'].mean():.1f} years\")\n",
    "        print(f\"Avg funding: ${tier_data['funding_total_usd'].mean():,.0f}\")\n",
    "        print(f\"Has funding: {tier_data['has_funding'].mean():.1%}\")\n",
    "        print(f\"Top statuses:\")\n",
    "        print(tier_data['status'].value_counts().head(3))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_optimized = create_optimized_targets()\n",
    "    analyze_risk_characteristics(df_optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc7f31c7"
   },
   "source": [
    "# Task\n",
    "Clean the dataset by handling missing values, standardizing categorical variables, and removing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ababcbac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values per column:\n",
      "parent_id              100.000000\n",
      "ROI                     99.630634\n",
      "last_investment_at      98.685851\n",
      "first_investment_at     98.685851\n",
      "invested_companies      98.681780\n",
      "investment_rounds       98.681780\n",
      "closed_at               98.667026\n",
      "short_description       96.371971\n",
      "funding_total_usd       85.818583\n",
      "last_funding_at         83.970227\n",
      "first_funding_at        83.970227\n",
      "funding_rounds          83.868473\n",
      "state_code              74.102151\n",
      "twitter_username        58.997828\n",
      "tag_list                58.559778\n",
      "lng                     57.338733\n",
      "lat                     57.338733\n",
      "city                    57.319400\n",
      "country_code            55.233448\n",
      "founded_at              53.586564\n",
      "milestones              53.346426\n",
      "last_milestone_at       53.346426\n",
      "first_milestone_at      53.346426\n",
      "description             53.168865\n",
      "logo_height             43.979486\n",
      "logo_url                43.979486\n",
      "logo_width              43.979486\n",
      "category_code           37.326828\n",
      "domain                  35.617874\n",
      "homepage_url            35.617874\n",
      "overview                35.401139\n",
      "relationships           34.029498\n",
      "created_by              20.869689\n",
      "age_group                0.800802\n",
      "normalized_name          0.013228\n",
      "name                     0.011702\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing_percentages = df_optimized.isnull().mean() * 100\n",
    "missing_percentages = missing_percentages.sort_values(ascending=False)\n",
    "\n",
    "print(\"Percentage of missing values per column:\")\n",
    "print(missing_percentages[missing_percentages > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped administrative columns: ['entity_type', 'entity_id', 'parent_id', 'Unnamed: 0.1', 'permalink']\n"
     ]
    }
   ],
   "source": [
    "admin_cols_to_drop = [\n",
    "    'entity_type', 'entity_id', 'parent_id',\n",
    "    'Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 1',\n",
    "    'permalink'\n",
    "]\n",
    "\n",
    "existing_admin_cols = [col for col in admin_cols_to_drop if col in df_optimized.columns]\n",
    "df_optimized = df_optimized.drop(columns=existing_admin_cols)\n",
    "print(f\"Dropped administrative columns: {existing_admin_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d849a3eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 5 columns with >50% missing values.\n",
      "Preserved critical columns: ['funding_total_usd', 'funding_rounds', 'first_funding_at', 'last_funding_at', 'country_code', 'state_code', 'city', 'region', 'lat', 'lng', 'category_code', 'description', 'overview', 'tag_list', 'investment_rounds', 'invested_companies', 'milestones', 'first_milestone_at', 'last_milestone_at', 'twitter_username', 'homepage_url', 'domain', 'closed_at']\n"
     ]
    }
   ],
   "source": [
    "# Now handle missing values - drop columns with missing percentage > 50% \n",
    "missing_percentages = df_optimized.isnull().mean() * 100\n",
    "missing_percentages = missing_percentages.sort_values(ascending=False)\n",
    "\n",
    "columns_to_drop = missing_percentages[missing_percentages > 50].index\n",
    "\n",
    "# Define critical columns to preserve even if they have high missing values\n",
    "critical_cols = [\n",
    "    # Funding-related\n",
    "    'funding_total_usd', 'funding_rounds', 'first_funding_at', 'last_funding_at',\n",
    "    \n",
    "    # Geographic data\n",
    "    'country_code', 'state_code', 'city', 'region', 'lat', 'lng',\n",
    "    \n",
    "    # Company descriptive info (valuable for analysis)\n",
    "    'category_code', 'description', 'overview', 'tag_list',\n",
    "    \n",
    "    # Investment/milestone data\n",
    "    'investment_rounds', 'invested_companies', 'milestones',\n",
    "    'first_milestone_at', 'last_milestone_at',\n",
    "    \n",
    "    # Company metadata\n",
    "    'twitter_username', 'homepage_url', 'domain', 'closed_at'\n",
    "]\n",
    "\n",
    "columns_to_drop = columns_to_drop.difference(critical_cols)\n",
    "\n",
    "df_optimized = df_optimized.drop(columns=columns_to_drop)\n",
    "print(f\"Dropped {len(columns_to_drop)} columns with >50% missing values.\")\n",
    "print(f\"Preserved critical columns: {[col for col in critical_cols if col in df_optimized.columns]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2bc5f437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 196,553 rows\n",
      "\n",
      "Columns with missing values:\n",
      "invested_companies    193962\n",
      "investment_rounds     193962\n",
      "closed_at             193933\n",
      "funding_total_usd     168679\n",
      "last_funding_at       165046\n",
      "first_funding_at      165046\n",
      "funding_rounds        164846\n",
      "state_code            145650\n",
      "twitter_username      115962\n",
      "tag_list              115101\n",
      "lat                   112701\n",
      "lng                   112701\n",
      "city                  112663\n",
      "country_code          108563\n",
      "first_milestone_at    104854\n",
      "last_milestone_at     104854\n",
      "milestones            104854\n",
      "description           104505\n",
      "logo_height            86443\n",
      "logo_width             86443\n",
      "logo_url               86443\n",
      "category_code          73367\n",
      "homepage_url           70008\n",
      "domain                 70008\n",
      "overview               69582\n",
      "relationships          66886\n",
      "created_by             41020\n",
      "age_group               1574\n",
      "normalized_name           26\n",
      "name                      23\n",
      "dtype: int64\n",
      "Dropped 23 rows with missing name (truly critical)\n",
      "\n",
      "After intelligent cleaning: 196,530 rows\n",
      "Data preservation rate: 100.3%\n",
      "\n",
      "Remaining missing values (this is NORMAL and OK):\n",
      "invested_companies    193941\n",
      "investment_rounds     193941\n",
      "closed_at             193911\n",
      "funding_total_usd     168657\n",
      "last_funding_at       165025\n",
      "first_funding_at      165025\n",
      "funding_rounds        164825\n",
      "state_code            145629\n",
      "twitter_username      115945\n",
      "tag_list              115087\n",
      "lng                   112686\n",
      "lat                   112686\n",
      "city                  112648\n",
      "country_code          108550\n",
      "first_milestone_at    104839\n",
      "last_milestone_at     104839\n",
      "milestones            104839\n",
      "description           104492\n",
      "logo_height            86431\n",
      "logo_width             86431\n",
      "logo_url               86431\n",
      "category_code          73357\n",
      "homepage_url           69997\n",
      "domain                 69997\n",
      "overview               69565\n",
      "relationships          66877\n",
      "created_by             41017\n",
      "age_group               1574\n",
      "normalized_name            4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# FIXED: Handle missing values intelligently without dropping most of the data\n",
    "print(f\"Starting with {len(df_optimized):,} rows\")\n",
    "\n",
    "# Check what columns still have missing values\n",
    "remaining_missing = df_optimized.isnull().sum()\n",
    "remaining_missing = remaining_missing[remaining_missing > 0].sort_values(ascending=False)\n",
    "print(f\"\\nColumns with missing values:\")\n",
    "print(remaining_missing)\n",
    "\n",
    "# ONLY drop rows for truly critical missing data\n",
    "# Most companies won't have closed_at (they're still operating!)\n",
    "# Most companies won't have funding dates (they're bootstrapped!)\n",
    "critical_only_cols = ['name']  # Only name is truly critical\n",
    "\n",
    "for col in critical_only_cols:\n",
    "    if col in df_optimized.columns:\n",
    "        rows_before = len(df_optimized)\n",
    "        df_optimized.dropna(subset=[col], inplace=True)\n",
    "        rows_after = len(df_optimized)\n",
    "        print(f\"Dropped {rows_before - rows_after:,} rows with missing {col} (truly critical)\")\n",
    "\n",
    "print(f\"\\nAfter intelligent cleaning: {len(df_optimized):,} rows\")\n",
    "print(f\"Data preservation rate: {len(df_optimized)/196000:.1%}\")\n",
    "\n",
    "# Final check\n",
    "final_missing = df_optimized.isnull().sum()\n",
    "final_missing = final_missing[final_missing > 0].sort_values(ascending=False)\n",
    "print(f\"\\nRemaining missing values (this is NORMAL and OK):\")\n",
    "print(final_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 168,657 missing funding_total_usd values with $0\n"
     ]
    }
   ],
   "source": [
    "# Missing funding likely means $0 funding\n",
    "if 'funding_total_usd' in df_optimized.columns:\n",
    "    funding_before = df_optimized['funding_total_usd'].isnull().sum()\n",
    "    df_optimized['funding_total_usd'] = df_optimized['funding_total_usd'].fillna(0)\n",
    "    print(f\"Imputed {funding_before:,} missing funding_total_usd values with $0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 164,825 missing funding_rounds values with 0\n",
      "Left 165,025 missing first_funding_at values as NaN (no funding events)\n",
      "Left 165,025 missing last_funding_at values as NaN (no funding events)\n"
     ]
    }
   ],
   "source": [
    "# Handle other funding-related columns\n",
    "funding_cols_to_fill = ['funding_rounds', 'first_funding_at', 'last_funding_at']\n",
    "for col in funding_cols_to_fill:\n",
    "    if col in df_optimized.columns:\n",
    "        if col == 'funding_rounds':\n",
    "            # Missing funding rounds = 0 rounds\n",
    "            before_count = df_optimized[col].isnull().sum()\n",
    "            df_optimized[col] = df_optimized[col].fillna(0)\n",
    "            print(f\"Imputed {before_count:,} missing {col} values with 0\")\n",
    "        else:\n",
    "            # For date columns, leave as NaN (indicates no funding events)\n",
    "            print(f\"Left {df_optimized[col].isnull().sum():,} missing {col} values as NaN (no funding events)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 193,941 missing investment_rounds values with 0\n",
      "Imputed 193,941 missing invested_companies values with 0\n"
     ]
    }
   ],
   "source": [
    "# Handle investment-related columns\n",
    "investment_cols_to_fill = ['investment_rounds', 'invested_companies']\n",
    "for col in investment_cols_to_fill:\n",
    "    if col in df_optimized.columns:\n",
    "        before_count = df_optimized[col].isnull().sum()\n",
    "        df_optimized[col] = df_optimized[col].fillna(0)\n",
    "        print(f\"Imputed {before_count:,} missing {col} values with 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 104,839 missing milestones values with 0\n"
     ]
    }
   ],
   "source": [
    "# Handle milestone columns\n",
    "milestone_cols = ['milestones']\n",
    "for col in milestone_cols:\n",
    "    if col in df_optimized.columns:\n",
    "        before_count = df_optimized[col].isnull().sum()\n",
    "        df_optimized[col] = df_optimized[col].fillna(0)\n",
    "        print(f\"Imputed {before_count:,} missing {col} values with 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 108,550 missing country_code values with 'Unknown'\n",
      "Imputed 145,629 missing state_code values with 'Unknown'\n",
      "Imputed 0 missing region values with 'Unknown'\n",
      "Imputed 112,648 missing city values with 'Unknown'\n",
      "Imputed 73,357 missing category_code values\n",
      "Imputed 104,492 missing description values\n",
      "Imputed 69,565 missing overview values\n",
      "Imputed 115,087 missing tag_list values\n",
      "Imputed 115,945 missing twitter_username values with 'None'\n",
      "Imputed 69,997 missing homepage_url values with 'None'\n",
      "Imputed 69,997 missing domain values with 'None'\n",
      "Imputed 66,877 missing relationships values with 'Unknown'\n",
      "Imputed 41,017 missing created_by values with 'Unknown'\n",
      "Keeping 112,686 missing lat values as NaN (no fake coordinates)\n",
      "Keeping 112,686 missing lng values as NaN (no fake coordinates)\n",
      "Imputed 86,431 missing logo_height values with median: 105.0\n",
      "Imputed 86,431 missing logo_width values with median: 267.0\n",
      "Dropped 0 rows with missing values in 'name'.\n",
      "\n",
      "Percentage of missing values per column after cleaning:\n",
      "closed_at             98.667379\n",
      "last_funding_at       83.969369\n",
      "first_funding_at      83.969369\n",
      "lat                   57.337811\n",
      "lng                   57.337811\n",
      "first_milestone_at    53.345036\n",
      "last_milestone_at     53.345036\n",
      "logo_url              43.978527\n",
      "age_group              0.800896\n",
      "normalized_name        0.002035\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "geographic_cols = ['country_code', 'state_code', 'region', 'city']\n",
    "for col in geographic_cols:\n",
    "    if col in df_optimized.columns:\n",
    "        before_count = df_optimized[col].isnull().sum()\n",
    "        df_optimized[col] = df_optimized[col].fillna('Unknown')\n",
    "        print(f\"Imputed {before_count:,} missing {col} values with 'Unknown'\")\n",
    "\n",
    "text_cols = ['category_code', 'description', 'overview', 'tag_list']\n",
    "for col in text_cols:\n",
    "    if col in df_optimized.columns:\n",
    "        before_count = df_optimized[col].isnull().sum()\n",
    "        if col == 'category_code':\n",
    "            df_optimized[col] = df_optimized[col].fillna('other')\n",
    "        else:\n",
    "            df_optimized[col] = df_optimized[col].fillna('Unknown')\n",
    "        print(f\"Imputed {before_count:,} missing {col} values\")\n",
    "\n",
    "web_cols = ['twitter_username', 'homepage_url', 'domain']\n",
    "for col in web_cols:\n",
    "    if col in df_optimized.columns:\n",
    "        before_count = df_optimized[col].isnull().sum()\n",
    "        df_optimized[col] = df_optimized[col].fillna('None')\n",
    "        print(f\"Imputed {before_count:,} missing {col} values with 'None'\")\n",
    "\n",
    "relationship_cols = ['relationships', 'created_by']\n",
    "for col in relationship_cols:\n",
    "    if col in df_optimized.columns:\n",
    "        before_count = df_optimized[col].isnull().sum()\n",
    "        df_optimized[col] = df_optimized[col].fillna('Unknown')\n",
    "        print(f\"Imputed {before_count:,} missing {col} values with 'Unknown'\")\n",
    "\n",
    "numerical_cols = ['lat', 'lng', 'logo_height', 'logo_width']\n",
    "for col in numerical_cols:\n",
    "    if col in df_optimized.columns:\n",
    "        before_count = df_optimized[col].isnull().sum()\n",
    "        if col in ['lat', 'lng']:\n",
    "            # For coordinates, keep as NaN\n",
    "            print(f\"Keeping {before_count:,} missing {col} values as NaN (no fake coordinates)\")\n",
    "        else:\n",
    "            median_value = df_optimized[col].median()\n",
    "            df_optimized[col] = df_optimized[col].fillna(median_value)\n",
    "            print(f\"Imputed {before_count:,} missing {col} values with median: {median_value}\")\n",
    "\n",
    "critical_missing_cols = ['name']\n",
    "for col in critical_missing_cols:\n",
    "    if col in df_optimized.columns:\n",
    "        rows_before = len(df_optimized)\n",
    "        df_optimized.dropna(subset=[col], inplace=True)\n",
    "        rows_after = len(df_optimized)\n",
    "        print(f\"Dropped {rows_before - rows_after} rows with missing values in '{col}'.\")\n",
    "\n",
    "\n",
    "# Verify remaining missing values\n",
    "missing_percentages_after = df_optimized.isnull().mean() * 100\n",
    "missing_percentages_after = missing_percentages_after.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nPercentage of missing values per column after cleaning:\")\n",
    "print(missing_percentages_after[missing_percentages_after > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "845adab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns identified:\n",
      "Index(['id', 'name', 'normalized_name', 'category_code', 'status', 'closed_at',\n",
      "       'domain', 'homepage_url', 'twitter_username', 'logo_url', 'description',\n",
      "       'overview', 'tag_list', 'country_code', 'state_code', 'city', 'region',\n",
      "       'first_funding_at', 'last_funding_at', 'first_milestone_at',\n",
      "       'last_milestone_at', 'relationships', 'created_by', 'updated_at',\n",
      "       'risk_tier_label'],\n",
      "      dtype='object')\n",
      "\n",
      "Unique values for 'category_code':\n",
      "['web' 'games_video' 'network_hosting' 'advertising' 'cleantech' 'other'\n",
      " 'enterprise' 'consulting' 'mobile' 'health' 'software' 'analytics'\n",
      " 'finance' 'education' 'medical' 'manufacturing' 'biotech' 'ecommerce'\n",
      " 'public_relations' 'hardware' 'search' 'news' 'government' 'security'\n",
      " 'photo_video' 'travel' 'semiconductor' 'social' 'legal' 'transportation'\n",
      " 'hospitality' 'sports' 'nonprofit' 'fashion' 'messaging' 'music'\n",
      " 'automotive' 'design' 'real_estate' 'local' 'nanotech' 'pets']\n",
      "\n",
      "Unique values for 'country_code':\n",
      "['USA' 'Unknown' 'MAR' 'IND' 'AUS' 'FRA' 'JPN' 'NLD' 'EGY' 'ISR' 'GBR'\n",
      " 'THA' 'CAN' 'AUT' 'IRL' 'SWE' 'DEU' 'BRA' 'FIN' 'RUS' 'SGP' 'MEX' 'CHN'\n",
      " 'ESP' 'ISL' 'KOR' 'TUR' 'DNK' 'ARG' 'PAK' 'HUN' 'POL' 'GRC' 'PRT' 'BLR'\n",
      " 'CSS' 'MKD' 'CHE' 'SVN' 'UKR' 'ITA' 'NZL' 'LIE' 'NOR' 'CZE' 'VNM' 'HRV'\n",
      " 'BEN' 'CHL' 'GHA']\n",
      "... and 126 more.\n",
      "\n",
      "Unique values for 'state_code':\n",
      "['WA' 'CA' 'Unknown' 'NY' 'NM' 'TX' 'OH' 'NJ' 'IL' 'MA' 'NC' 'CT' 'PA'\n",
      " 'NH' 'MI' 'AZ' 'TN' 'GA' 'FL' 'MD' 'UT' 'OR' 'MO' 'AR' 'CO' 'KS' 'MN'\n",
      " 'NV' 'ID' 'IN' 'IA' 'VA' 'KY' 'LA' 'HI' 'WV' 'WI' 'SC' 'NE' 'DE' 'DC'\n",
      " 'AL' 'VT' 'RI' 'OK' 'ME' 'MS' 'MT' 'SD' 'ND']\n",
      "... and 2 more.\n",
      "\n",
      "Unique values for 'region':\n",
      "['Seattle' 'Los Angeles' 'SF Bay' 'unknown' 'Agadir' 'Vadodara' 'New York'\n",
      " 'Santa Fe' 'San Diego' 'Austin' 'Abbotsford' 'New Delhi' 'Columbus'\n",
      " 'New Jersey - Other' 'Chicago' 'Langhrone Creek' 'West Bridgewater'\n",
      " 'Houston' 'Charlotte' 'Paris' 'Shinagawa-ku' 'Amsterdam' 'Wilton'\n",
      " 'Philadelphia' 'Boston' 'Bangalore' 'Cairo' 'Tel Aviv' 'Portsmouth'\n",
      " 'Detroit' 'Iselin' 'London' 'Niantic' 'Amherst' 'Bangkok' 'Phoenix'\n",
      " 'New York - Other' 'Gatineau' 'Vienna' 'Mansfield' 'Cork' 'Nashville'\n",
      " 'Atlanta' 'Hattem' 'Liverpool' 'Leeds' 'Jacksonville' 'TBD' 'Kingston'\n",
      " 'Fort Lauderdale']\n",
      "... and 5797 more.\n",
      "\n",
      "Unique values for 'status':\n",
      "['operating' 'acquired' 'closed' 'ipo']\n",
      "\n",
      "Unique values for 'age_group':\n",
      "['15+y', '8-15y', NaN]\n",
      "Categories (4, object): ['0-3y' < '4-7y' < '8-15y' < '15+y']\n",
      "\n",
      "Unique values for 'risk_tier_label':\n",
      "['low_risk' 'high_risk' 'medium_risk']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df_optimized.select_dtypes(include='object').columns\n",
    "print(\"Categorical columns identified:\")\n",
    "print(categorical_cols)\n",
    "\n",
    "# Inspect unique values for potential inconsistencies in a few key categorical columns\n",
    "cols_to_inspect = ['category_code', 'country_code', 'state_code', 'region', 'status', 'age_group', 'risk_tier_label']\n",
    "\n",
    "for col in cols_to_inspect:\n",
    "    if col in df_optimized.columns:\n",
    "        print(f\"\\nUnique values for '{col}':\")\n",
    "        # Display a limited number of unique values if there are many\n",
    "        unique_values = df_optimized[col].unique()\n",
    "        if len(unique_values) > 50:\n",
    "            print(unique_values[:50])\n",
    "            print(f\"... and {len(unique_values) - 50} more.\")\n",
    "        else:\n",
    "            print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f92b4622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized 'category_code'.\n",
      "Standardized 'country_code'.\n",
      "Standardized 'state_code'.\n",
      "Standardized 'region'.\n",
      "Standardized 'status'.\n",
      "Standardized 'risk_tier_label'.\n",
      "\n",
      "Unique values for 'category_code' after standardization:\n",
      "['web' 'games_video' 'network_hosting' 'advertising' 'cleantech' 'other'\n",
      " 'enterprise' 'consulting' 'mobile' 'health' 'software' 'analytics'\n",
      " 'finance' 'education' 'medical' 'manufacturing' 'biotech' 'ecommerce'\n",
      " 'public_relations' 'hardware' 'search' 'news' 'government' 'security'\n",
      " 'photo_video' 'travel' 'semiconductor' 'social' 'legal' 'transportation'\n",
      " 'hospitality' 'sports' 'nonprofit' 'fashion' 'messaging' 'music'\n",
      " 'automotive' 'design' 'real_estate' 'local' 'nanotech' 'pets']\n",
      "\n",
      "Unique values for 'country_code' after standardization:\n",
      "['usa' 'unknown' 'mar' 'ind' 'aus' 'fra' 'jpn' 'nld' 'egy' 'isr' 'gbr'\n",
      " 'tha' 'can' 'aut' 'irl' 'swe' 'deu' 'bra' 'fin' 'rus' 'sgp' 'mex' 'chn'\n",
      " 'esp' 'isl' 'kor' 'tur' 'dnk' 'arg' 'pak' 'hun' 'pol' 'grc' 'prt' 'blr'\n",
      " 'css' 'mkd' 'che' 'svn' 'ukr' 'ita' 'nzl' 'lie' 'nor' 'cze' 'vnm' 'hrv'\n",
      " 'ben' 'chl' 'gha']\n",
      "... and 126 more.\n",
      "\n",
      "Unique values for 'state_code' after standardization:\n",
      "['wa' 'ca' 'unknown' 'ny' 'nm' 'tx' 'oh' 'nj' 'il' 'ma' 'nc' 'ct' 'pa'\n",
      " 'nh' 'mi' 'az' 'tn' 'ga' 'fl' 'md' 'ut' 'or' 'mo' 'ar' 'co' 'ks' 'mn'\n",
      " 'nv' 'id' 'in' 'ia' 'va' 'ky' 'la' 'hi' 'wv' 'wi' 'sc' 'ne' 'de' 'dc'\n",
      " 'al' 'vt' 'ri' 'ok' 'me' 'ms' 'mt' 'sd' 'nd']\n",
      "... and 2 more.\n",
      "\n",
      "Unique values for 'region' after standardization:\n",
      "['seattle' 'los angeles' 'sf bay' 'unknown' 'agadir' 'vadodara' 'new york'\n",
      " 'santa fe' 'san diego' 'austin' 'abbotsford' 'new delhi' 'columbus'\n",
      " 'new jersey - other' 'chicago' 'langhrone creek' 'west bridgewater'\n",
      " 'houston' 'charlotte' 'paris' 'shinagawa-ku' 'amsterdam' 'wilton'\n",
      " 'philadelphia' 'boston' 'bangalore' 'cairo' 'tel aviv' 'portsmouth'\n",
      " 'detroit' 'iselin' 'london' 'niantic' 'amherst' 'bangkok' 'phoenix'\n",
      " 'new york - other' 'gatineau' 'vienna' 'mansfield' 'cork' 'nashville'\n",
      " 'atlanta' 'hattem' 'liverpool' 'leeds' 'jacksonville' 'tbd' 'kingston'\n",
      " 'fort lauderdale']\n",
      "... and 5780 more.\n",
      "\n",
      "Unique values for 'status' after standardization:\n",
      "['operating' 'acquired' 'closed' 'ipo']\n",
      "\n",
      "Unique values for 'risk_tier_label' after standardization:\n",
      "['low_risk' 'high_risk' 'medium_risk']\n",
      "\n",
      "Unique values for 'age_group':\n",
      "['15+y', '8-15y', NaN]\n",
      "Categories (4, object): ['0-3y' < '4-7y' < '8-15y' < '15+y']\n"
     ]
    }
   ],
   "source": [
    "# Standardize categorical columns: convert to lowercase and strip whitespace\n",
    "cols_to_standardize = ['category_code', 'country_code', 'state_code', 'region', 'status', 'risk_tier_label']\n",
    "\n",
    "for col in cols_to_standardize:\n",
    "    if col in df_optimized.columns and df_optimized[col].dtype == 'object':\n",
    "        df_optimized[col] = df_optimized[col].str.lower().str.strip()\n",
    "        print(f\"Standardized '{col}'.\")\n",
    "\n",
    "# Re-verify unique values after standardization\n",
    "for col in cols_to_standardize:\n",
    "    if col in df_optimized.columns:\n",
    "        print(f\"\\nUnique values for '{col}' after standardization:\")\n",
    "        unique_values = df_optimized[col].unique()\n",
    "        if len(unique_values) > 50:\n",
    "            print(unique_values[:50])\n",
    "            print(f\"... and {len(unique_values) - 50} more.\")\n",
    "        else:\n",
    "            print(unique_values)\n",
    "\n",
    "# age_group is already a categorical type with defined labels, no string standardization needed.\n",
    "print(\"\\nUnique values for 'age_group':\")\n",
    "print(df_optimized['age_group'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6ac65433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows found: 0\n",
      "No duplicate rows to remove.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df_optimized.duplicated()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.sum()\n",
    "\n",
    "print(f\"Number of duplicate rows found: {num_duplicates}\")\n",
    "\n",
    "# Remove duplicate rows if any exist\n",
    "if num_duplicates > 0:\n",
    "    rows_before_dropping = len(df_optimized)\n",
    "    df_optimized.drop_duplicates(inplace=True)\n",
    "    rows_after_dropping = len(df_optimized)\n",
    "    print(f\"Removed {rows_before_dropping - rows_after_dropping} duplicate rows.\")\n",
    "else:\n",
    "    print(\"No duplicate rows to remove.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimized.to_csv('./companies_cleaned_targets.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
